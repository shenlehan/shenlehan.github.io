<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2026新年展望</title>
    <url>/2026/01/01/2026%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/</url>
    <content><![CDATA[<p>2026 就这么突然来了。</p>
<p>我已经很久没有写什么非技术性的东西了，只能感叹这一年过的比我体感要快得多。</p>
<p>2025 的一月仿佛还历历在目。在鼓楼图书馆里看 FSF，百无聊赖地复习着 CPL
和微积分；南京清早熙熙攘攘的人声，冬日清朗的空气，教超的蛋糕和布丁仿佛还是几天前的事情。</p>
<p>是因为长大一点了吗，对于过去的事情记得更加清楚，但时间也感觉流动的更快了。</p>
<p>去年寒假看了一半的
CSAPP，到现在还没看完。学了一点皮毛的数学分析，只有在看花书的时候派上了点用场。CS61A
倒是学完了，但是感觉这门课属于会的人不需要听，不会的人一开始也难解其妙。</p>
<p>先争取活过这个无比劳碌的期末周吧。</p>
<p>寒假里最重要的还是在学校里老老实实搬砖，争取今年能把第一篇文章投中（能中
TPAMI 什么的就最好了）。</p>
<p>然后完成一些一直想学，但是也没时间学的东西吧：</p>
<ol type="1">
<li><p>UCB CS186 Database System：正好学学 Java，万一将来不去做 AI
去做互联网码农也能用得上。
其实我一直对软工系的东西很感兴趣，但苦于课程安排一直没什么机会。</p></li>
<li><p>UCB CS168/Stanford CS144 Computer
Networking：其实早在一年前就听完了 USTC
的计网，但是当时也就听个响，现在脑海里也就一点粗略的印象。
看网上的建议，听 UCB 的课，然后把两个的实验都做了。
还是得动手写点代码：（</p></li>
<li><p>把拖了一年的 CSAPP 听完，然后把实验做了。其实我很想做计科的
ICSPA，但是不知道有没有时间，估计要做的话只有过年的时间能砸进去了</p></li>
<li><p>操作系统。这个还没决定是 MIT 的还是 UCB
的，先把上面的七七八八做完再考虑这个吧。</p></li>
<li><p>学学 JavaScript</p></li>
<li><p>把《统计学习方法》之前跳过的部分读完。这学期学了凸优化，应该算是补上了
AI 需要的数学中的最后一块了。</p></li>
</ol>
<p>这个学期都没写什么代码。事实上，整个一年都没写什么代码，但是理论貌似学的也不怎么样。</p>
<p>还是希望能提升一下自己的动手能力，亲自做一个大一点的工程吧。</p>
<p>再见，2025；干杯，2026！</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title>Computer Networking 1--Introduction</title>
    <url>/2026/01/14/Computer-Networking-1-Introduction/</url>
    <content><![CDATA[<h2 id="概览">概览</h2>
<p>我们将网络架构自顶向下地分成 5
层。每一层只需要做好自己的工作就可以了。</p>
<p>由于历史原因，最顶层被叫做 "Layer 7"</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/layers.png" class="">
<h2 id="layers-of-the-internet">Layers of the Internet</h2>
<h3 id="physical-layer">Physical Layer</h3>
<p>In the Internet, we’re looking for a way to signal bits (1s and 0s)
across space.</p>
<p>The technology could be voltages on an electrical wire, wireless
radio waves, light pulses along optical fiber cables, among others.</p>
<p>Physical Layer 数据的基本单位是<strong>bit</strong>。</p>
<h3 id="link-layer">Link Layer</h3>
<p>Link Layer cares about how to send the data using Physical Layer.</p>
<p>Link Layer
只负责相邻节点之间的通信，其单位是<strong>frame/帧</strong>。</p>
<h3 id="internet-layer">Internet Layer</h3>
<p>Internet Layer 是路由器工作的最高层，负责决定 next
hop，即把数据转发给谁。其基本单位是<strong>packet/包</strong>。</p>
<p>IP 协议就是在这一层工作的。</p>
<h3 id="transport-layer">Transport Layer</h3>
<p>Transport Layer 负责决定数据传输的可靠性，也就是 TCP/UDP
协议，以及“端到端通信”。</p>
<p>也就是说，他通过端口区分应用的数据的来/去。</p>
<p>其基本单位是<strong>段/segment</strong>。</p>
<h3 id="application-layer">Application Layer</h3>
<p>只负责考虑如何使用网络。它的基本数据单位被称作
<strong>message</strong>。</p>
<h2 id="headers">Headers</h2>
<p>每一层都会把从更低层拿来的数据加上对应的 Header。</p>
<p>Header 是只给这一层看的信息（同时，每一层也只能看自己层级的
header），它指明了信息传输的各种附加信息（采取的协议、收件人/发件人等等）。</p>
<p>在传输过程中，Header 不断被 peel off，然后又被加上新的。</p>
<p>由于 Header 的存在，每一层相当于仅仅和自己同层的 peer
进行通信，因此也要求相同层级采用的协议必须相同。</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/header1.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/header2.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/Header.png" class="">
<h2 id="resource-sharing">Resource Sharing</h2>
<p>网络的总容量是有限度的，因此如何分配资源是一个重要的问题。</p>
<p>我们先考虑两个问题：</p>
<ol type="1">
<li><p>要保证网络正常工作，我们至少要多大的容量？</p></li>
<li><p>我们如何分配我们的容量？</p></li>
</ol>
<p>第一个问题的解决方案被称作 statistical
multiplexing，其原理非常直观：</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/sm.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/sm1.png" class="">
<p>第二个问题有两种解决办法，一种被称作 <strong>best
effort</strong>，通俗而言就是所有人都只管发送信息，并且 "hope for the
best"，其对应的策略被称作 packet switching</p>
<p>另一种被称作
<strong>reservation</strong>，也就是在通信前会先在网络中预留出容量，其对应的策略被称作
Circuit Switching</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/reservation.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/reservation1.png" class="">
<h2 id="properties-of-links">Properties of Links</h2>
<p>The <strong>bandwidth</strong> of a link tells us how many bits we
can send on the link per unit time. Intuitively, this is the speed of
the link. If you think of a link as a pipe carrying water, the bandwidth
is the width of the pipe. A wider pipe lets us feed more water into the
pipe per second. We usually measure bandwidth in bits per second (e.g. 5
Gbps = 5 billion bits per second).</p>
<p>The <strong>propagation delay</strong> of a link tells us how long it
takes for a bit to travel along the link. In the pipe analogy, this is
the length of the link. A shorter pipe means that water spends less time
in the pipe before arriving at the other end. Propagation delay is
measured in time (e.g. nanoseconds, milliseconds).</p>
<p>If we multiply the bandwidth and the propagation delay, we get the
<strong>bandwidth-delay product (BDP)</strong>. Intuitively, this is the
<strong>capacity</strong> of the link, or the number of bits that exist
on the link at any given instant. In the pipe analogy, if we fill up the
pipe and freeze time, the capacity of the pipe is how much water is in
the pipe in that instant.</p>
<p>注意，bandwidth/带宽是指“一秒内发射的 bit 数量”，而 bit
必须是一个一个发射出去的。所以一个 bit 的发射用时是 <span
class="math inline">\(\frac{1}{\text{bandwidth}}\)</span>。</p>
<p>注意区分 delay（或者用全称，propagation delay） 和这个发射用时
<strong>（也就是上文的 transmission delay）</strong> 的区别。前者是 bit
在物理链路里面传播(propagation)的时间，后者是终端发送一个 packet 所有
bit 的时间。这从我们下文的例子就可以看出来。</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/bandwidth.png" class="">
<h3 id="timing-diagram">Timing Diagram</h3>
<p>Suppose we have a link with bandwidth 1 Mbps = 1 million bits per
second, and propagation delay of 1 ms = 0.001 seconds.</p>
<p>We want to send a 100 byte = 800 bit packet along this link. How long
does it take to send this packet, from the time the first bit is sent,
to the time the last bit is received?</p>
<p>To answer this question, we can draw a timing diagram. The left bar
is the sender, and the right bar is the recipient. Time starts at 0 and
increases as we move down the diagram.</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/timing_diagram.png" class="">
<p>Let’s focus on the first bit. We can put 1,000,000 bits on the link
per second (bandwidth), so it takes 1/1,000,000 = 0.000001 seconds to
put a single bit on the link. At time 0.000001 seconds, the link has a
single bit on it, at the sender end.</p>
<p>It then takes 0.001 seconds for this bit to travel across the link
(propagation delay), so at time 0.000001 + 0.001 seconds, the very first
bit arrives at the recipient.</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/timing_diagram1.png" class="">
<p>Now let’s think about the last bit. From before, it takes 0.000001 to
put a bit on the link. We have 800 bits to send, so the last bit is
placed on the link at time <span
class="math inline">\(800*0.000001=0.0008\)</span> seconds.</p>
<p>It then takes 0.001 seconds for the last bit to travel across the
link, so at time 0.0008 + 0.001 seconds, the very last bit arrives at
the recipient. This is the time when we can say the packet has arrived
at the recipient.</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/timing_diagram2.png" class="">
<h3 id="packet-delay">Packet Delay</h3>
<p>从上文例子可以看出，一个完整的 packet 发送的延迟就是 <span
class="math inline">\(\text{transmission delay + propagation
delay}\)</span></p>
<h3 id="pipe-diagram">Pipe Diagram</h3>
<p>To draw the link, we can imagine the link is a pipe (similar to the
water analogy) and draw the pipe as a rectangle, where the width is the
propagation delay, and the height is the bandwidth. The area of the pipe
is the capacity of the link.</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/pipe1.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/pipe2.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/pipe3.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/pipe4.png" class="">
<p>Pipe diagrams can be useful for comparing different links. Let’s look
at the exact same packets traveling through three different links.</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/pipe5.png" class="">
<h3 id="overloaded-links">Overloaded Links</h3>
<p>In the long term, we have enough capacity to send all the outgoing
packets, but at this very instant in time, we have two packets arriving
simultaneously, and we can only send out one. This is called transient
overload, and it’s extremely common at switches in the Internet.</p>
<p>To cope with transient overload, the switch maintains a queue of
packets. If two packets arrive simultaneously, the switch queues one of
them and sends out the other one.</p>
<img src="/2026/01/14/Computer-Networking-1-Introduction/queue1.png" class="">
<img src="/2026/01/14/Computer-Networking-1-Introduction/queue2.png" class="">
<p>At any given time, the switch could choose to send a packet from one
of the incoming links, or send a packet from the queue. This choice is
determined by a packet scheduling algorithm, and there are lots of
different designs that we’ll look at.</p>
<p>Now that we have a notion of queuing, we need to go back and update
our packet delay formula. <strong>Now, packet delay is the sum of
transmission delay, propagation delay, and queuing delay</strong>.</p>
]]></content>
      <categories>
        <category>Computer Network</category>
      </categories>
      <tags>
        <tag>CS168</tag>
      </tags>
  </entry>
  <entry>
    <title>Computer Networking 2--Routing</title>
    <url>/2026/01/15/Computer-Networking-2-Routing/</url>
    <content><![CDATA[<h2 id="what-is-routing">What is Routing?</h2>
<h3 id="end-host-and-router">End Host and Router</h3>
<p>我们先说明 End Host 和 Router 的区别。</p>
<p>前者是网络中一条线路的端点，他只负责接受、发送自己的信息，而不负责“传送”任务。
后者则相反，仅仅起到中介的作用。</p>
<h3 id="packets">Packets</h3>
<p>我们之前说过，Packets 是 Internet Layer 传送的基本单元。</p>
<p>为了正确地在互联网中传送数据，我们显然需要指明 Source 和
Destination。 他们需要一个唯一的标识符。</p>
<p>这个标识符是怎么来的，姑且先不考虑。在有了这个标识符之后，我们便在
packet 的头部加上 Source Address 和 Destination Address 两个元素。</p>
<img src="/2026/01/15/Computer-Networking-2-Routing/packets.png" class="">
<h2 id="routing-states">Routing States</h2>
<h3 id="what-is-rouing-state">What is Rouing State</h3>
<p>Routing States 最简单的说，是一种策略。即当 router 接收到一个 packet
的时候，根据这些策略，便能成功地把信息传送到目的地。</p>
<p>其中，最重要的、也是最泛用的策略被称作转发表/Forward Table</p>
<h3 id="forwarding-table">Forwarding Table</h3>
<p>理论上而言，我们可以建立一张表：这张表直白的告诉我们，如果我想把一个
packet 送到一个地方，我的 next hop 应该是什么。</p>
<img src="/2026/01/15/Computer-Networking-2-Routing/ft.png" class="">
<p>在现实中，next hop 往往不是指一个抽象的地点，而是指物理上的一个
port</p>
<img src="/2026/01/15/Computer-Networking-2-Routing/ft1.png" class="">
<h3 id="routing-vs.-forwarding">Routing vs. Forwarding</h3>
<p>我们现在区分两个概念：routing 本身指的是填充 forwarding table
的过程，而 forwarding 指的才是转发 packet 的过程。</p>
<p>因此，forwarding 本身并不需要知道 forwarding table
是如何计算的，故它是一个局部的动作。</p>
<p>相反，routing
本身要求我们必须对于整个网络结构有一个认知，因此这是一个全局的过程。</p>
<h2 id="distance-vector-protocols">Distance Vector Protocols</h2>
<p>Distance Vector Protocols 的主要内容如下：</p>
<div class="note "><p>For each destination:</p>
<ul>
<li>If you hear an advertisement for that destination, update the table
and reset the TTL if:
<ul>
<li>The destination isn’t in the table.</li>
<li>The advertised cost, plus the link cost to the neighbor, is better
than the best-known cost.</li>
<li>The advertisement is from the current next-hop. Includes poison
advertisements.</li>
</ul></li>
<li>Advertise to all your neighbors when the table updates, and
periodically (advertisement interval).
<ul>
<li>But don’t advertise back to the next-hop.</li>
<li>…Or, advertise poison back to the next-hop.</li>
<li>Any cost greater than or equal to 16 is advertised as infinity.</li>
</ul></li>
<li>If a table entry expires, make the entry poison and advertise
it.</li>
</ul>
</div>
<p>我们分条来看。</p>
<h3 id="update-rules">Update Rules</h3>
<p>我们如果从毗邻的节点接收到一条 <code>(from, to, cost, TTL)</code>
的信息，我们进行如下的判定来决定要不要更新我们的 forwarding table:</p>
<ol type="1">
<li>最明显的，如果我根本没有到 to 的路径，那我肯定要加上这条</li>
<li>这条路径比我现在手上有的要更快，我肯定选择它更好</li>
<li>这条路径是从我当前的最优的 next hop 发来的，这说明 next hop
有更新，它到 to
的距离发生了变化，有可能不再是最优解。为了保证正确性，我现在先更新这条，等后面别人广播的时候自然会更新成正确答案。</li>
<li>或者，如果是 <code>poison</code> 的，也就是 “我无法到达某个目的地”
的信息，那么我收到的 Cost 就是无穷大，类似 3，我也要更新。</li>
</ol>
<h3 id="advertise-rules">Advertise Rules</h3>
<p>我们通过不停广播自己的信息来使得这个网络达到收敛态/converge</p>
<ol type="1">
<li>首先，我们必须定时广播信息，告诉我的邻居们我还在线，以及我到某个目的地的最小
cost 是多少</li>
<li>但是注意，我们不能反向告诉 next hop，否则，如果 next hop
失效（不再能够到达目的地），它会误以为通过我还能抵达目的地，造成死循环（这被称作
Split Horizon 原则）</li>
<li>或者，我们定时广播时，反向告诉 next hop，我不可以到达 A（即，我是从
next hop 来的，我不能把 packet 再返回给你）。这被称作 Poison
Reverse，往往是比 Split Horizon 更有效的广播方式</li>
<li>如果我们已经有一个死循环了，我们该怎么办？方法很简单，给定一个上限，如果
cost 超过了这个上限，我们就认为他是无穷大/不可达的。</li>
</ol>
<img src="/2026/01/15/Computer-Networking-2-Routing/advertise.png" class="" title="advertise">
<h2 id="link-state-protocols">Link-State Protocols</h2>
<h3 id="overview">Overview</h3>
<div class="note "><p>Link-state protocols in one sentence: Every router <strong>learns the
full network graph</strong>, and then <strong>runs shortest-paths on the
graph</strong> to populate the forwarding table.</p>
</div>
<p>直白地说，Link-state
协议通过让所有路由器都有全局信息，使得他们并行计算（通过
Bellman-Ford/Dijkstra 等最短路算法）出自己的next hop
但这显然引起了一个问题，如果 A 计算的最短路径和 B
计算出的路径不一致怎么办呢？</p>
<p>因此，我们做出以下 4 条约定：</p>
<div class="note "><ol type="1">
<li><p>All routers have to agree on the network topology. Suppose a link
failed, but only one router knows about it. Then different routers are
computing paths on totally different graphs, and might produce
inconsistent results.</p></li>
<li><p>All routers are finding least-cost paths through the path. If one
router preferred more expensive paths for some reason, we would get
inconsistent results.</p></li>
<li><p>All costs are positive. Negative costs could produce
negative-weight cycles.</p></li>
<li><p>All routers use the same tiebreaking rules. If we assumed
shortest paths are unique, then the previous two conditions are
sufficient to ensure everybody picks the same path. This condition
additionally ensures that if there are multiple paths tied as the
shortest, everyone chooses the same one.</p></li>
</ol>
</div>
<h3 id="learning-about-graph-topology">Learning About Graph
Topology</h3>
<ol type="1">
<li><p>To discover neighbors, every router sends a hello message to all
of its neighbors. For example, in this network, R2 sends to both of its
neighbors: “Hello, I’m R2.” Now, R1 knows that it’s connected to R2, and
R3 also knows that it’s connected to R2. Similarly, R1 says hello to R2,
so now R2 knows about R1. Likewise, R3 says hello to R2, so R2 also
knows about R3.</p>
<p>As a result, everybody now knows who their immediate neighbors are.
Note that R1 does not know about R3, because R1 and R3 are not
neighbors.</p></li>
</ol>
<p><img src="/2026/01/15/Computer-Networking-2-Routing/graph3.png" class=""></p>
<p><img src="/2026/01/15/Computer-Networking-2-Routing/graph1.png" class=""></p>
<ol type="1">
<li>Now that we know about our neighbors, we should announce that fact
to everybody. To make a global announcement, we send the announcement to
all of our neighbors. Also, if we ever receive an announcement, we
should send it to all of our neighbors as well. This ensures that every
message gets propagated throughout the network. This is known as
flooding information across the network. If any information changes
(e.g. a neighbor disappears), we should flood that information as
well.</li>
</ol>
<img src="/2026/01/15/Computer-Networking-2-Routing/graph2.png" class="">
<ol start="3" type="1">
<li>avoid infinite flooding: When we see a message for the first time,
send that message to all neighbors, and write down that we’ve seen that
message. (We have to write down this message anyway, since we’re trying
to use this information to build up the network graph.) Then, if we ever
see that same message again, don’t send it a second time.</li>
</ol>
]]></content>
      <categories>
        <category>Computer Network</category>
      </categories>
      <tags>
        <tag>CS168</tag>
      </tags>
  </entry>
  <entry>
    <title>Computer Networking 3--Transport</title>
    <url>/2026/01/20/Computer-Networking-3-Transport/</url>
    <content><![CDATA[<h2 id="overview">Overview</h2>
<h3 id="demultiplexing-with-ports">Demultiplexing with Ports</h3>
<p>IP 协议只能看到 IP。如果一台主机上同时有两个应用在和一个 server
通信，那么二者在 IP 层面是完全一样的（有相同的 source/destination
IP）</p>
<p>我们该如何区分这两个应用，将数据正确的交给他们？</p>
<p>解决方案就是 port，每个应用走不同的 port 出去。</p>
<img src="/2026/01/20/Computer-Networking-3-Transport/port1.png" class="">
<img src="/2026/01/20/Computer-Networking-3-Transport/port2.png" class="">
<div class="note "><p>Because the transport layer is implemented in the <strong>operating
system</strong>, these ports (sometimes called <strong>logical
ports</strong>) are the attachment point where the application connects
to the operating system’s network stack.</p>
</div>
<h2 id="tcp">TCP</h2>
]]></content>
      <categories>
        <category>Computer Network</category>
      </categories>
      <tags>
        <tag>CS168</tag>
      </tags>
  </entry>
  <entry>
    <title>EM算法</title>
    <url>/2025/12/20/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="em-算法的目标">EM 算法的目标</h2>
<p>先考虑著名的三硬币问题： <div class="note primary"><p>假设三枚硬币 A, B, C，其正面朝上的概率分别为 <span
class="math inline">\(\pi,p,q\)</span>。</p>
<p>先抛 A，若正面朝上抛 B，反之则抛 C。</p>
<p>只记录抛 B，C 的结果，得到一个 01 序列。</p>
<p>要求给出此模型的参数 <span
class="math inline">\(\theta=(\pi,p,q)\)</span></p>
</div></p>
<p>抽象出问题实质：</p>
<p>给出观测数据 <span class="math inline">\(Y=[Y_1,\dots,Y_n]^T\)</span>
以及隐变量 <span
class="math inline">\(Z=[Z_1,\dots,Z_n]^T\)</span>，则给出似然估计：
<span class="math display">\[
P(Y|\theta) =\sum_Z P(Y|Z,\theta) P(Z|\theta)
\]</span></p>
<p>此问题中可以化简为：</p>
<p><span class="math display">\[
P(Y|\theta) =\prod_{j=1}^n \left[\pi p^{y_j} (1-p)^{1-y_j} +(1-\pi)
q^{y_j} (1-q)^{1-y_j}  \right]
\]</span> 希望给出</p>
<p><span class="math display">\[
\hat{\theta}=\arg \max_{\theta} \log(P(Y|\theta))
\]</span></p>
<p>下面给出一个用语的约定：</p>
<ul>
<li><p><strong>观测数据</strong>/<strong>不完全数据</strong> <span
class="math inline">\(Y=[Y_1,\dots,Y_n]^T\)</span></p></li>
<li><p>隐变量/<strong>未观测数据</strong>/ <span
class="math inline">\(\text{Latent Variables}\)</span> <span
class="math inline">\(Z=[Z_1,\dots,Z_n]^T\)</span></p></li>
<li><p>完全数据：<span
class="math inline">\(P(Y,Z|\theta)\)</span></p></li>
<li><p>完全数据的对数似然函数：<span class="math inline">\(\log
P(Y,Z|\theta)\)</span></p></li>
</ul>
<h2 id="em-算法的导出">EM 算法的导出</h2>
<p>设 <span class="math display">\[
\begin{align}
L(\theta) &amp;=P(Y|\theta)=\sum_Z P(Y|Z,\theta) P(Z|\theta) \\
&amp;=\log \left( \sum_Z P(Y|Z,\theta) P(Z|\theta) \right)
\end{align}
\]</span></p>
<p>我们逐步迭代去求出最优化的 <span
class="math inline">\(\theta\)</span>，设第 <span
class="math inline">\(i\)</span> 次求出的解是 <span
class="math inline">\(\theta^{(i)}\)</span>，则： <span
class="math display">\[
\begin{align}
L(\theta)-L(\theta^{(i)})&amp;=\log \left( \sum_Z P(Y|Z,\theta)
P(Z|\theta) \right)-\log P(Y|\theta^{(i)})
\\
&amp;= \log \left(\sum_Z P(Z|Y,\theta^{(i)}) \dfrac{  P(Y|Z,\theta)
P(Z|\theta) }
{P(Z|Y,\theta^{(i)})} \right)-\log P(Y|\theta^{(i)})\\
&amp;\geq \sum_Z P(Z|Y,\theta^{(i)})\log \left( \dfrac{  P(Y|Z,\theta)
P(Z|\theta) }
{P(Z|Y,\theta^{(i)})}   \right)-\log P(Y|\theta^{(i)})\\
&amp;= \sum_Z P(Z|Y,\theta^{(i)})\log \left( \dfrac{  P(Y|Z,\theta)
P(Z|\theta) }
{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}   \right)\\
\end{align}
\]</span></p>
<p>不等号是Jensen不等式给出的</p>
<p>设 <span
class="math inline">\(B(\theta,\theta^{(i)})\hat{=}L(\theta^{(i)})+\sum_Z
P(Z|Y,\theta^{(i)})\log \left( \dfrac{ P(Y|Z,\theta) P(Z|\theta)
}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})} \right)\)</span></p>
<p>显然有 <span class="math inline">\(L(\theta)\geq
B(\theta,\theta^{(i)})\)</span>，即 <span
class="math inline">\(B\)</span> 为原本函数的一个下界，且 <span
class="math inline">\(\theta=\theta^{(i)}\)</span> 时取等。
故可以考虑最大化 <span class="math inline">\(B\)</span>，从而使得 <span
class="math inline">\(L\)</span> 最大。</p>
<p>即： <span class="math display">\[
\begin{align}
\theta^{(i+1)} &amp;= \arg \max _{\theta} B(\theta,\theta^{(i)})  \\
&amp;=\arg \max_{\theta} \left( L(\theta^{(i)})+\sum_Z
P(Z|Y,\theta^{(i)})\log \left( \dfrac{  P(Y|Z,\theta) P(Z|\theta)
}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}   \right) \right)
\\
&amp;=\arg \max_{\theta} \left( \sum_Z P(Z|Y,\theta^{(i)})\log
\left(   P(Y|Z,\theta) P(Z|\theta) \right) \right)\\
&amp;=\arg \max_{\theta} \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)
\end{align}
\]</span> 上面的等号都是去掉了常数项（上面的变量只有 <span
class="math inline">\(\theta\)</span>，<span
class="math inline">\(\theta^{(i)}\)</span>固定） 令 <span
class="math inline">\(Q(\theta,\theta^{(i)})=\sum_Z
P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)\)</span>，则 Q 函数即为：</p>
<p>完全似然函数 <span class="math inline">\(\log P(Y,Z|\theta)\)</span>
关于在给定观测数据 <span class="math inline">\(Y\)</span>
和当前参数估计<span class="math inline">\(\theta^{(i)}\)</span>
下，对于未观测数据 <span class="math inline">\(Z\)</span> 的条件概率分布
<span class="math inline">\(P(Z|Y,\theta^{(i)})\)</span> 的期望。</p>
<p>EM 算法实质上就是找一个凸函数 <span
class="math inline">\(B/Q\)</span>，让它在 <span
class="math inline">\(\theta^{(i)}\)</span> 处严格等于对数似然函数 <span
class="math inline">\(\log
P(Y,Z|\theta)\)</span>，但始终在似然函数下方。我们通过不断增大 B
函数，迫使似然函数上升。</p>
<div class="note info"><p>关于 Q 函数更直观的认知，见 <a
href="https://chat.qwen.ai/c/41591c41-69ab-4005-8cbd-a9db5412b27b">Qwen</a></p>
</div>
<h2 id="em-算法的流程">EM 算法的流程</h2>
<ol type="1">
<li>初始化 <span class="math inline">\(\theta^{(0)}\)</span></li>
<li>E(Expectation): 求出 Q 函数</li>
<li>M(Maximization): 最大化 Q 函数，求出下一轮的 <span
class="math inline">\(\theta^{(i+1)}\)</span></li>
<li>重复 E-M 步，直到收敛为止</li>
</ol>
<h2 id="gmm">GMM</h2>
<p>GMM，即 Gaussian Mixture Model，刻画了如下的问题： <div class="note "><p>有 K 个 Gaussian，分别服从 <span
class="math inline">\(\theta_k=(\mu_k,\sigma_k)\)</span>；第 <span
class="math inline">\(k\)</span> 个模型被选择的概率是 <span
class="math inline">\(\alpha_k\)</span></p>
<p>即 <span class="math inline">\(P(y)=\sum_{k=1}^K \alpha_k
\phi(\mu_k,\sigma_k)\)</span></p>
</div></p>
<p>现在你既不知道各个Gaussian的具体参数，也不知道选择的概率 <span
class="math inline">\(\alpha_k\)</span>，任务就是根据观测的序列 <span
class="math inline">\(y=(y_1,\dots,y_N)\)</span> 求出 <span
class="math inline">\(\theta=(\alpha_1,\dots,\alpha_K;\theta_1,\dots,\theta_K)\)</span></p>
<p>使用 EM 算法完成这个任务，逐步分析：</p>
<h3 id="明确隐变量">明确隐变量</h3>
<p>设 <span class="math display">\[
\gamma_{jk}=
\begin{cases}
1 &amp; 第 j 个观测来自第 k 个模型\\
0 &amp; 反之
\end{cases}
\]</span></p>
<p>有了观测数据 <span class="math inline">\(y_j\)</span>
之后，那么完全数据就是</p>
<p><span class="math display">\[
(y_j,\gamma_{j1},\dots,\gamma_{jk})
\]</span></p>
<p>后面整体算作上文的 <span class="math inline">\(Z\)</span></p>
<p>写出似然函数：</p>
<p>$$ <span class="math display">\[\begin{align}
P(y,\gamma|\theta) &amp;= \prod_{j=1}^N
P(y_j,\gamma_{j1}\dots,\gamma_{jk})\\

&amp;= \prod_{j=1}^N \prod_{k=1}^K [\alpha_k
\phi(y_j|\theta_k)]^{\gamma_{jk}}\\

\end{align}\]</span> $$</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>EM 算法</tag>
      </tags>
  </entry>
  <entry>
    <title>First Blog</title>
    <url>/2025/12/19/First-Blog/</url>
    <content><![CDATA[<p>Hello! This is my first blog.</p>
<p>A math formula: <span class="math display">\[
\frac{\partial p_t(\mathbf{x})}{\partial t} = -\sum_{i=1}^{D}
\frac{\partial}{\partial x_i} \left[ f_i(\mathbf{x}, t) p_t(\mathbf{x})
\right] + \frac{1}{2} \sum_{i=1}^{D} \sum_{j=1}^{D}
\frac{\partial^2}{\partial x_i \partial x_j} \left[ (g^2(t)
\mathbf{I}_D)_{ij} p_t(\mathbf{x}) \right]
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;This is a piece of code!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>尝试一下中文输入 This's my blog!</p>
<ul>
<li>todo 1</li>
<li>todo 2</li>
</ul>
<h1 id="hello">Hello</h1>
<h2 id="你好">你好</h2>
<h3 id="foo"><code>foo</code></h3>
<h4 id="geq-2"><span class="math inline">\(1+2\geq 2\)</span></h4>
<p>有<span class="math inline">\(a+b\geq 3\)</span></p>
<p><span class="math display">\[a+b\geq 3\]</span></p>
]]></content>
  </entry>
  <entry>
    <title>Hexo Blog 的一些技术细节</title>
    <url>/2025/12/20/Hexo%20Blog%20%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/</url>
    <content><![CDATA[<p>花了一些时间总算搞好了，第一篇文章就简要记录一下踩的坑吧。</p>
<h2 id="数学公式渲染">数学公式渲染</h2>
<p>处于兼容性考虑，我采用了 <code>Mathjax</code> 作为 math
engine，同时将默认 <code>Markdown</code> 渲染器换成了
<code>pandoc</code></p>
<p>一定要把之前的渲染引擎删除干净，否则会有一些很诡异的错误</p>
<h2 id="英文引号变成中文">英文引号变成中文</h2>
<p>我想写这篇文章的罪魁祸首。</p>
<p>一开始，所有的英文引号 <code>'/"</code> 都会被渲染成
<code>‘/“</code>，显示效果奇差无比</p>
<p>查询之后发现这也是渲染引擎的问题，它会自动开启一个类似于 formatting
之类的操作</p>
<p>Github 上比较常见的几个 <a
href="https://github.com/hexojs/hexo/issues/1981">issue</a> 都是用的
<code>hexo-renderer-marked</code></p>
<p>但是如上文所说，我换成了 <code>pandoc</code>
之后依然有这个问题（有的文章说换 <code>pandoc</code>
之后就好了），猜测也有一个类似的开关</p>
<p>随问 LLM，发现要传入一些参数： <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">pandoc:</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;--from&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;markdown-smart&#x27;</span> <span class="comment"># 这里的 -smart 表示在 markdown 解析中禁用 smart 扩展</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;--mathjax&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>我估计可能是因为 pandoc 版本的不同，旧版本的默认关闭/没有 smart
扩展</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title>Mamba Architecture</title>
    <url>/2025/12/24/Mamba-Architecture/</url>
    <content><![CDATA[<p>Mamba 是一种 State Space
Model/SSM，即状态空间模型。这是用以处理序列问题的一种非常优秀的模型。</p>
<p>其本质可以理解为“隐空间”/“状态空间”中的递推。</p>
<h2 id="问题">问题</h2>
<p>给定时间信号输入序列 <span
class="math inline">\(x(t)\)</span>，以及输出序列 <span
class="math inline">\(y(t)\)</span>，尝试给出预测模型。
我们考虑一个隐状态空间 <span class="math inline">\(h(t)\in
\mathbb{R}^N\)</span>，它由 <span class="math inline">\(x(t)\)</span>
而来，控制了 <span class="math inline">\(y(t)\)</span> 的输出。
从简化的角度考虑，给出线性模型：</p>
<p><span class="math display">\[\begin{align}
h&#39;(t) &amp;= \mathbf{A}h(t)+\mathbf{B}x(t) \\
y(t)  &amp;= \mathbf{C}h&#39;(t)
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{A}\)</span>，<span
class="math inline">\(\mathbf{B}\)</span>，<span
class="math inline">\(\mathbf{C}\)</span> 三个矩阵分别代表 evolution
parameter 和 projection
parameter。第一个的作用是将隐空间中的状态逐步演化的最终态，后二者的作用是进行隐空间和实际输入/输出空间的映射。</p>
<h2 id="从连续时间到离散时间">从连续时间到离散时间</h2>
<p>上面的模型显然是一个连续时间模型，也就意味着是一个微分方程。</p>
<p>但是在实际工程和应用中，我们不可能去解一个微分方程，因此我们考虑去将他离散化。其中，最重要的一种离散化方式被称作
"Zero-Order Hold/ZOH" 规则。</p>
<h3 id="zoh">ZOH</h3>
<p>ZOH 的核心假设非常易于理解：</p>
<div class="note "><h4 id="zero-order-hold">Zero-Order Hold</h4>
<p>在每个采样区间中，输入保持常数：</p>
<p><span class="math display">\[
u(t)=u_k\quad t\in [t_k,t_{k+1})
\]</span></p>
<p>其中，采样步长 <span
class="math inline">\(\Delta&gt;0\)</span>，<span
class="math inline">\(t_k=k\Delta\)</span></p>
</div>
<p>我们希望得到离散递推公式： <span class="math display">\[
x_{k+1}=\bar{A}x_k+\bar{B}u_k
\]</span></p>
<p>所以，也就是去解这个微分方程（和普通的微分方程一样，先不考虑是矩阵的情况）：
<span class="math display">\[
\frac{\mathrm{d}x}{\mathrm{d}t}=Ax+Bu
\]</span></p>
<p>注意，这里 <span class="math inline">\(x\)</span> 是状态，<span
class="math inline">\(u\)</span> 是输入。</p>
<p>将系统重写为 <span class="math display">\[
\dot{x}(t)-Ax(t)=Bu(t)
\]</span></p>
<p>解之即有： <span class="math display">\[
\bar{A}=e^{\Delta
A},\bar{B}=\int_{t_k}^{t_{k+1}}e^{A(t_{k+1}-\tau)}B\mathrm{d}\tau
\]</span></p>
<p>若 <span class="math inline">\(A\)</span> 可逆，则 <span
class="math inline">\(B\)</span> 有闭式解 <span
class="math inline">\(A^{-1}(e^{A\Delta}-I)B\)</span></p>
<div class="note info"><h4 id="矩阵指数">矩阵指数</h4>
<p><span class="math inline">\(B=e^{A}\)</span> 并不是 <span
class="math inline">\(B_{ij}=\exp\{a_{ij}\}\)</span>，而是采用泰勒展开：
<span class="math display">\[
e^A=\sum_{i=0}^{\infin} \frac{A^k}{k!}
\]</span></p>
<p>对于对角矩阵，退化为上面的直觉的形式： <span class="math display">\[
A=\text{diag}(a_1,\dots,a_n) \Rightarrow
e^A=\text{diag}(e^{a_1},\dots,e^{a_n})
\]</span></p>
<p>类似，对于可对角化矩阵，也有： <span class="math display">\[
A=P \text{diag}(a_1,\dots,a_n) P^{-1} \Rightarrow
e^A=P\text{diag}(e^{a_1},\dots,e^{a_n})P^{-1}
\]</span></p>
</div>
<h3 id="递推">递推</h3>
<p>基于 ZOH 假设，我们可以快速写出以下式子（<span
class="math inline">\(M\)</span> 是序列长度）： <span
class="math display">\[
\begin{align}
\bar{K} &amp;= (C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B},\dots,
C\bar{A}^{M-1}\bar{B})\\
y &amp;= x * \bar{K}
\end{align}
\]</span></p>
<h2 id="一个-mamba-block">一个 Mamba Block</h2>
<p>一个 Mamba Block
做一次完整的我们上述的递推操作，也就是对于每个位置提取一次上下文理解。</p>
<p>和 Transformer 类似，我们也可以把多个 Mamba Block
叠起来，进行更深度的信息提取，也就是<strong>逐层抽象</strong>，越靠后的
Mamba Block 蕴含了越丰富的、越抽象的信息。</p>
<p><span class="math display">\[
x^{(0)}\xrightarrow{\text{Mamba Block 1}}
x^{(1)}\xrightarrow{\text{Mamba Block 2}}\dots \xrightarrow{\text{Mamba
Block M}} x^{(M)}
\]</span></p>
<h2 id="具体实现">具体实现</h2>
<h3 id="algorithm-1parameters-function">Algorithm 1：Parameters
Function</h3>
<p>根据我们上文的描述，从给定的序列 <span
class="math inline">\(\{x\}\)</span> 中生成 <span
class="math inline">\(\bar{A}, \bar{B}, C\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
\textbf{Algorithm 1: Parameters Function}\\
\textbf{Require: } x&#39; \in \mathbb{R}^{(B,N,P)}\\
\textbf{Ensure: } \bar A \in \mathbb{R}^{(B,N,P,K)},\ \bar B \in
\mathbb{R}^{(B,N,P,K)},\ C \in \mathbb{R}^{(B,N,K)}\\[4pt]
\begin{array}{ll}
1: &amp; B \in \mathbb{R}^{(B,N,K)} \leftarrow
\mathrm{Linear}^{B}(x&#39;)\\
2: &amp; C \in \mathbb{R}^{(B,N,K)} \leftarrow
\mathrm{Linear}^{C}(x&#39;)\\
3: &amp; \Delta \in \mathbb{R}^{(B,N,P)} \leftarrow
\log\!\Big(1+\exp(\mathrm{Linear}^{\Delta}(x&#39;)+\mathrm{Parameter}^{\Delta})\Big)\\
4: &amp; \text{// } \mathrm{Parameter}^{A} \in \mathbb{R}^{(P,K)}\\
5: &amp; \bar A \in \mathbb{R}^{(B,N,P,K)} \leftarrow \Delta \otimes
\mathrm{Parameter}^{A}\\
6: &amp; \bar B \in \mathbb{R}^{(B,N,P,K)} \leftarrow \Delta \otimes B\\
\textbf{Return:} &amp; \bar A,\ \bar B,\ C
\end{array}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\textbf{Algorithm 2: Mamba Block}\\
\textbf{Require: } T_{l-1} \in \mathbb{R}^{(B,N,C)}\\
\textbf{Ensure: } T_{l} \in \mathbb{R}^{(B,N,C)}\\[4pt]
\begin{array}{ll}
1: &amp; \text{// Apply layer normalization to } T_{l-1}\\
2: &amp; T&#39;_{l-1} \in \mathbb{R}^{(B,N,C)} \leftarrow
\mathrm{Norm}(T_{l-1})\\
3: &amp; x \in \mathbb{R}^{(B,N,P)} \leftarrow
\mathrm{Linear}^{x}(T&#39;_{l-1})\\
4: &amp; z \in \mathbb{R}^{(B,N,P)} \leftarrow
\mathrm{Linear}^{z}(T&#39;_{l-1})\\
5: &amp; \text{// Process the input sequence}\\
6: &amp; x&#39; \in \mathbb{R}^{(B,N,P)} \leftarrow
\mathrm{SiLU}(\mathrm{Conv1d}(x))\\
7: &amp; \bar A,\ \bar B,\ C \leftarrow
\mathrm{ParametersFunction}(x&#39;)\\
8: &amp; y \in \mathbb{R}^{(B,N,P)} \leftarrow \mathrm{SSM}(\bar A,\bar
B,C)(x&#39;)\\
9: &amp; \text{// Obtain gated } y\\
10: &amp; y&#39; \in \mathbb{R}^{(B,N,P)} \leftarrow y \odot
\mathrm{SiLU}(z)\\
11: &amp; \text{// Residual connection}\\
12: &amp; T_{l} \in \mathbb{R}^{(B,N,C)} \leftarrow
\mathrm{Linear}^{T}(y&#39;) + T_{l-1}\\
\textbf{Return:} &amp; T_{l}
\end{array}
\end{aligned}
\]</span></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Mamba</tag>
      </tags>
  </entry>
  <entry>
    <title>Pre-norm and Post-norm</title>
    <url>/2026/01/14/Pre-norm-and-Post-norm/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>RWKV 初探</title>
    <url>/2026/01/27/RWKV-%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h2 id="introduction">Introduction</h2>
<p>我们熟知，深度学习架构的发展，最开始的代表性结构是循环神经网络/RNN。</p>
<p>RNN 占用内存少，但是它梯度消失问题十分严重，而且难以并行训练。</p>
<p>接下来出现了 Transformer
架构，它效果极佳，但是由于上下文依赖的问题，它的时间复杂度达到了 <span
class="math inline">\(\Theta(n^2)\)</span>，这严重限制了他在长上下文的表现。</p>
<p>一种新的模型，即我们将要看到的 Receptance Weighted Key Value
(RWKV)模型，则尝试合并了二者的优点。</p>
<h2 id="rwkv">RWKV</h2>
<h3 id="overview">Overview</h3>
<p>RWKV 架构提出了 4 个基本向量，他们内嵌于所有的块之中，即：</p>
<ol type="1">
<li><p>R: The <strong>Receptance</strong> vector acts as the receiver of
past information. 充当过去信息的“接收器” 。在公式里，它通常通过 <span
class="math inline">\(sigmoid\)</span> 函数变成一个门控信号 。</p></li>
<li><p>W: The <strong>Weight</strong> signifies the positional weight
decay vector, a trainable parameter within the model.
这是一个可训练的参数，代表位置权重衰减向量
。它决定了过去的信息随时间“遗忘”的速度 。</p></li>
<li><p>K: The <strong>Key</strong> vector performs a role analogous to K
in traditional attention mechanisms. 其角色类似于传统 Transformer
注意力机制中的 <span class="math inline">\(K\)</span></p></li>
<li><p>V : The <strong>Value</strong> vector functions similarly to V in
conventional attention processes. 同样类似于传统注意力机制中的 <span
class="math inline">\(V\)</span></p></li>
</ol>
<h3 id="architecture">Architecture</h3>
<img src="/2026/01/27/RWKV-%E5%88%9D%E6%8E%A2/rwkv1.png" class="">
<p>RWKV 模型由多个堆叠的残差块 (Stacked Residual Blocks) 组成
。每个残差块包含两个子模块：</p>
<ol type="1">
<li><p>时间混合 (Time-mixing)
子块：负责处理序列在时间轴上的依赖关系，利用上述四个要素来捕捉过去的信息</p></li>
<li><p>通道混合 (Channel-mixing)
子块：负责在单个时间步内处理不同通道特征的交互。</p></li>
</ol>
<p>RWKV Block 很像于注意力机制。</p>
<h4 id="token-shift">Token Shift</h4>
<p>在进入这四个向量的线性变换之前，RWKV
引入了一个非常简单的动作：它将当前步的输入 <span
class="math inline">\(x_t\)</span> 与上一步的输入 <span
class="math inline">\(x_{t-1}\)</span> 进行线性插值 。</p>
<p>这种设计让模型在计算每一层时都能“瞥一眼”过去，从而实现标记位移 。
具体的计算公式如下：</p>
<p><span class="math display">\[
r_t = W_r \cdot (\mu_r \odot x_t + (1 - \mu_r) \odot x_{t-1})\\
k_t = W_r \cdot (\mu_k \odot x_t + (1 - \mu_k) \odot x_{t-1})\\
v_t = W_r \cdot (\mu_v \odot x_t + (1 - \mu_v) \odot x_{t-1})
\]</span></p>
<h3 id="process">Process</h3>
<p>我们完整梳理一下一个 RWKV Block 的流程。</p>
<h4 id="time-mixing-channel">Time mixing channel</h4>
<ol type="1">
<li><p>归一化与位移（LayerNorm &amp; Token Shift）</p>
<ul>
<li><p>输入向量 <span class="math inline">\(x_t\)</span> 首先经过
LayerNorm 。</p></li>
<li><p>接着进行 Token Shift：将当前时间步的输入 <span
class="math inline">\(x_t\)</span> 与上一个时间步保存下来的输入 <span
class="math inline">\(x_{t-1}\)</span> 进行线性插值
。这就像是让模型同时观察“现在”和“刚刚”发生的事情 。</p></li>
</ul></li>
<li><p>生成 R, K, V</p>
<ul>
<li>通过线性变换生成三个向量：Receptance (<span
class="math inline">\(r_t\)</span>)、Key (<span
class="math inline">\(k_t\)</span>) 和 Value (<span
class="math inline">\(v_t\)</span>)，公式就是上面给出的</li>
</ul></li>
<li><p>WKV 算子</p>
<ul>
<li><p>这是 RWKV
最特殊的地方。它通过一个复杂的加权公式，计算所有过去时间步的 <span
class="math inline">\(v\)</span> 的加权平均值 。</p></li>
<li><p>权重由两部分决定：当前步的 <span
class="math inline">\(k_t\)</span> 和随时间衰减的权重 <span
class="math inline">\(w\)</span>（Time Decay）</p></li>
<li><p>它的数学表达如下： <span class="math display">\[
wkv_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} \odot v_i + e^{u+k_t}
\odot v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w+k_i} + e^{u+k_t}}
\]</span></p></li>
</ul></li>
<li><p>输出门控（Output Gating）</p>
<ul>
<li><p>将 Receptance (<span class="math inline">\(r_t\)</span>) 经过
<span class="math inline">\(sigmoid\)</span>
激活后，作为“门控信号”去乘以 WKV 的结果。</p></li>
<li><p>最后通过一个线性变换 <span class="math inline">\(W_o\)</span>
输出，并与原始输入 <span class="math inline">\(x_t\)</span>
进行残差相加。</p></li>
</ul></li>
</ol>
<h4 id="channel-mixing">Channel-mixing</h4>
<ol type="1">
<li><p>再次位移与投影</p>
<ul>
<li><p>同样先做 LayerNorm 和 Token Shift 。</p></li>
<li><p>生成用于通道混合的 <span class="math inline">\(r&#39;_t\)</span>
和 <span class="math inline">\(k&#39;_t\)</span></p></li>
</ul></li>
<li><p>平方 ReLU 激活</p>
<ul>
<li>对 <span class="math inline">\(k&#39;_t\)</span>
进行线性变换后，使用 Squared ReLU（即 <span class="math inline">\(V&#39;
= W&#39;_v \cdot \max(k&#39;_t, 0)^2\)</span>）进行激活 。</li>
</ul></li>
<li><p>最终门控输出</p>
<ul>
<li>再次利用 <span class="math inline">\(sigmoid(r&#39;_t)\)</span>
作为门控，决定保留多少计算后的特征 。结果与第一阶段的输出进行残差相加
。</li>
</ul></li>
</ol>
<p>对比时间混合和通道混合，我们容易发现：</p>
<ul>
<li><p>WKV 算子起到了“上下文”的作用</p></li>
<li><p>整体仍然保持“线性”的特点</p></li>
</ul>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>RWKV</tag>
      </tags>
  </entry>
  <entry>
    <title>UCB CS168 lab2 Routing 小记</title>
    <url>/2026/01/27/UCB-CS168-lab2-Routing-%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[<p>简单记录一下我做下来的一些小坑。</p>
<h2 id="stage-2">Stage 2</h2>
<h3 id="正确的端口">正确的端口</h3>
<p>事实上，<code>handle_data_packet(self, packet, in_port)</code>
注释说明了是
<code>:param in_port: the port from which the packet arrived.</code>，但这个其实没有作用。我们关心的是我们当前这个
router 要把收到的 packet 向哪个端口转发，所以其实是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dst = packet.dst</span><br><span class="line">...</span><br><span class="line">dst_port = <span class="variable language_">self</span>.table[dst].port</span><br><span class="line">latency = <span class="variable language_">self</span>.table[dst].latency</span><br></pre></td></tr></table></figure>
<h2 id="stage-4">Stage 4</h2>
<h3 id="注意-latency-计算完整">注意 <code>latency</code> 计算完整</h3>
<p>我们在接收到邻居的广播后，注意他发送过来的 latency 只是他到目标 host
的 latency，而我们如果要通过它中转，还需要加上我们到它的 latency:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">latency = route_latency + <span class="variable language_">self</span>.ports.get_latency(port)</span><br><span class="line">new_entry = TableEntry(dst=route_dst, port=port,</span><br><span class="line">                        latency=latency, </span><br><span class="line">                        expire_time=api.current_time()+<span class="variable language_">self</span>.ROUTE_TTL)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="stage-5">Stage 5</h2>
<h3 id="字典删除问题">字典删除问题</h3>
<p>由于 <code>self.table</code>
是一个字典类型，而直接在遍历中删除字典元素会引发错误：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">RuntimeError: dictionary changed size during iteration</span><br></pre></td></tr></table></figure>
<p>比较推荐的做法是先收集一个应当删除的 <code>list</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">expire_list = []</span><br><span class="line"><span class="keyword">for</span> dst, entry <span class="keyword">in</span> <span class="variable language_">self</span>.table.items():</span><br><span class="line">  <span class="keyword">if</span> entry.expire_time &lt;= api.current_time():</span><br><span class="line">    expire_list.append(dst)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dst <span class="keyword">in</span> expire_list:</span><br><span class="line">  <span class="keyword">if</span> <span class="variable language_">self</span>.POISON_EXPIRED:</span><br><span class="line">    new_entry = TableEntry(dst=dst, port=<span class="variable language_">self</span>.table[dst].port,</span><br><span class="line">                            latency=INFINITY, expire_time=api.current_time()+<span class="variable language_">self</span>.ROUTE_TTL)</span><br><span class="line">    <span class="variable language_">self</span>.table[dst] = new_entry</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="variable language_">self</span>.table.pop(dst)</span><br></pre></td></tr></table></figure>
<h2 id="stage-67">Stage 6&amp;7</h2>
<p>关于处理 Split Horizon 和 Poison Reverse
有一个更聪明的写法，我们在遇到 <code>self.SPLIT_HORIZON</code> 为 True
的时候选择直接 continue，这样就避免了大量繁琐的 if 语句。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> port <span class="keyword">in</span> ports_update:</span><br><span class="line">  <span class="keyword">for</span> dst <span class="keyword">in</span> <span class="variable language_">self</span>.table.keys():</span><br><span class="line">    latency = <span class="variable language_">self</span>.table[dst].latency</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> port == <span class="variable language_">self</span>.table[dst].port:</span><br><span class="line">      <span class="keyword">if</span> <span class="variable language_">self</span>.SPLIT_HORIZON:</span><br><span class="line">        <span class="comment"># Split Horizon: don&#x27;t send anything at all</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      <span class="keyword">elif</span> <span class="variable language_">self</span>.POISON_REVERSE:</span><br><span class="line">        <span class="comment"># Poison Reverse: send it as INFINITY</span></span><br><span class="line">        latency = INFINITY</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> latency &gt; INFINITY:</span><br><span class="line">      latency = INFINITY</span><br><span class="line"></span><br><span class="line">    route = RoutePacket(destination=dst, latency=latency)</span><br><span class="line">    <span class="keyword">if</span> force <span class="keyword">or</span> <span class="variable language_">self</span>.history[port].get(dst, <span class="literal">None</span>) != latency:</span><br><span class="line">      <span class="variable language_">self</span>.send_route(port, dst, latency=latency)</span><br><span class="line">      <span class="variable language_">self</span>.history[port][dst] = latency</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h2 id="stage-10a">Stage 10A</h2>
<p>题干提示了要使用一个 <code>self.history</code>
的数据结构来记录每个端口向每个目的地上一次转发的 packet
的信息。我一开始使用了一个元组 <code>(port, dst)</code>，然后使用
<code>self.key: {(port, dst): RoutePacket}</code>
这样的键值对来存储。</p>
<p>但这样有几个问题：</p>
<ol type="1">
<li><p>写起来很繁琐</p></li>
<li><p>有冗余的信息，我们不需要保存完整的 packet，因为我们的 key 已经有
dst 了（RoutePacket 本身有 latency 和 dst 两个信息）</p></li>
</ol>
<p>综上，我们采用一个更良好的写法，即一个二重字典。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ...</span>):</span><br><span class="line">  <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">  <span class="variable language_">self</span>.history = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in send_routes</span></span><br><span class="line"><span class="keyword">if</span> force <span class="keyword">or</span> <span class="variable language_">self</span>.history[port].get(dst, <span class="literal">None</span>) != latency:</span><br><span class="line">  <span class="variable language_">self</span>.send_route(port, dst, latency=latency)</span><br><span class="line">  <span class="variable language_">self</span>.history[port][dst] = latency</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="defaultdict"><code>defaultdict</code></h3>
<p>对于一个普通的 Python 字典, 如果你访问一个不存在的键，它会直接报错
KeyError。 但 defaultdict(dict) 自带默认值：</p>
<p>当尝试访问 self.history[port] 时，如果这个 port
还没出现过，它不会报错，而是自动创建一个空的字典 {} 填在那里。</p>
<p>配套的，使用 <code>dict.get(key, None)</code>
可以安全的取出对应的值。</p>
]]></content>
      <categories>
        <category>Computer Network</category>
      </categories>
      <tags>
        <tag>CS168</tag>
        <tag>lab</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Mamba源码导读</title>
    <url>/2026/01/15/Vision-Mamba%E6%BA%90%E7%A0%81%E5%AF%BC%E8%AF%BB/</url>
    <content><![CDATA[
]]></content>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2025/12/20/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="决策树学习算法">决策树学习算法</h2>
<p>决策树的学习算法一共有三点： <div class="note primary"><p>特征选择，决策树生成，决策树剪枝</p>
</div></p>
<p>决策树的非叶子节点代表选择的一个划分标准（如，第 <span
class="math inline">\(i\)</span> 个分量 <span
class="math inline">\(x^{(i)}\)</span> 是否大于等于 <span
class="math inline">\(0\)</span>），每一个叶子节点代表一个类（样本属于这些类）</p>
<h2 id="特征选择">特征选择</h2>
<p>希望每一步选择的特征都是良好的，引入一个指标来计算：信息增益。</p>
<p>信息增益：得知 特征X 的信息而使得类 Y
的信息的不确定性减少的程度，即经验熵的改变量。 定义为： <span
class="math display">\[
g(D, A)=H(D)-H(D|A)
\]</span> 其中 <span class="math inline">\(D\)</span> 为数据集，<span
class="math inline">\(A\)</span> 为我们选择的特征，<span
class="math inline">\(H\)</span> 为经验熵函数。</p>
<p>我们希望 <span class="math inline">\(H\)</span> 越小越好，即每一步的
<span class="math inline">\(g(D,A)\)</span> 要大。</p>
<p>容易发现，当 <span class="math inline">\(A\)</span>
表征的特征有更多选项时更容易被选中，所以引入信息增益比： <span
class="math display">\[
g_R(D, A)=\dfrac{g(D,A)}{H_A(D)}
\]</span> 其中 <span class="math inline">\(H_A(D)\)</span> 为数据集
<span class="math inline">\(D\)</span> 关于特征 <span
class="math inline">\(A\)</span> 的熵 <span class="math display">\[
H_A(D)=-\sum_{i=1}^n \dfrac{|D_i|}{|D|}\log_2{\dfrac{|D_i|}{|D|}}
\]</span></p>
<h2 id="决策树生成">决策树生成</h2>
<p>一共两种算法，ID3 和 C4.5</p>
<p>ID3 算法依赖于信息增益，而 C4.5 则是信息增益比作为评价标准</p>
<p>流程：</p>
<ol type="1">
<li><p>给定数据集 <span class="math inline">\(D\)</span> 以及特征集
<span class="math inline">\(A\)</span> 阈值 <span
class="math inline">\(\epsilon\)</span></p></li>
<li><p>按照每一个特征，计算信息增益（比）；选取信息增益（比）最大的特征
<span class="math inline">\(A\)</span>，如果节点全部都属于同一类 <span
class="math inline">\(C_k\)</span>，则直接标记为<span
class="math inline">\(C_k\)</span> 类并返回</p></li>
<li><p>否则开始划分，按照 <span class="math inline">\(A\)</span>
的每一个可能的值 <span class="math inline">\(a_i\)</span> 将划分为 <span
class="math inline">\(D_i\)</span>，递归此过程，直到子节点全部属于一类或者信息增益（比）小于阈值
<span class="math inline">\(\epsilon\)</span> 或者特征集 <span
class="math inline">\(A\)</span> 为空。</p></li>
</ol>
<p>这两个都容易产生过拟合的问题，故考虑剪枝</p>
<h2 id="决策树剪枝">决策树剪枝</h2>
<p>实质上就是一种正则化手段，利用经验熵作为损失函数，叶子节点数作为正则化项。</p>
<p>定义决策树学习的损失函数为：</p>
<p><span class="math display">\[
C_\alpha(T)=\sum_{t=1}^{|T|} N_tH_t(T)+\alpha|T|
\]</span></p>
<p>其中经验熵函数为</p>
<p><span class="math display">\[
H_t(T)=-\sum_k \dfrac{N_{tk}}{N_t}\log \dfrac{N_{tk}}{N_t}
\]</span></p>
<p>其中，<span class="math inline">\(|T|\)</span>
为决策树的节点数（正则化项），<span class="math inline">\(t\)</span>
是子节点编号，<span class="math inline">\(N_t\)</span>
是每个叶节点的样本数，其中第 <span class="math inline">\(k\)</span>
类的有 <span class="math inline">\(N_{tk}\)</span> 个。</p>
<p>当 <span class="math inline">\(\alpha\)</span>
很小的时候，说明选择损失函数最小的模型，调大 <span
class="math inline">\(\alpha\)</span> 相当于限制（减小）模型复杂度。</p>
<p>剪枝算法： 1. 递归，计算每一个叶节点的经验熵</p>
<ol start="2" type="1">
<li><p>尝试将一个叶子节点缩到父节点，比较前后的经验熵，如果经验熵减小，说明可以直接剪枝，将父节点变为一个新的叶节点。</p></li>
<li><p>回到2，继续操作</p></li>
</ol>
<h2 id="cart-决策树">CART 决策树</h2>
<h3 id="概述">概述</h3>
<p>CART 是一种二叉决策树，每次提问都是 y/n；</p>
<p>对于回归问题，使用平方误差作为损失函数；对于分类问题，使用 <span
class="math inline">\(\text{Gini}\)</span> 系数；</p>
<h3 id="模型">模型</h3>
<h4 id="回归树">回归树</h4>
<p><span class="math display">\[
f(x)=\sum_{m=1}^M c_m \mathbb{I}(x\in R_m)
\]</span></p>
<p>其中，<span class="math inline">\(R_m\)</span>
为提问所划分出的区域，<span class="math inline">\(c_m\)</span>
为其上的代表值（固定的输出值）；</p>
<p>通过平方误差计算时，容易知道 <span class="math inline">\(c_m\)</span>
应该取 <span class="math inline">\(R_m\)</span> 的标签的平均值</p>
<p>现在问题在于如何划分。</p>
<p>采用启发式的方法，每次取一个方向 <span
class="math inline">\(x^{(j)}\)</span>，考虑将 D
按照这个方向上的一个数值 <span class="math inline">\(s\)</span>
来划分</p>
<p><span class="math display">\[
R_1(j,s)=\{x|x^{(j)}\leq s\},R_2(j,s)=\{x|x^{(j)}&gt; s\}
\]</span></p>
<p>现在来求解</p>
<p><span class="math display">\[
\min_{j,s} \left\{ \min_{c_1} \sum_{x_i\in R_1(j,s)} (y_i-c)^2 +
\sum_{x_i\in R_2(j,s)} (y_i-c)^2 \right\}
\]</span> 固定 <span class="math inline">\(j\)</span> 有 <span
class="math inline">\(\hat{c}_1=\text{ave}(y_i|x_i\in
R_1(j,s)),\hat{c}_2=\text{ave}(y_i|x_i\in R_2(j,s))\)</span></p>
<p>重复这个过程，直到达到终止条件。</p>
<h4 id="分类树">分类树</h4>
<p>定义 <span class="math inline">\(\text{Gini}\)</span>
系数，对于一个概率分布 <span class="math inline">\(p\)</span> 有</p>
<p><span class="math display">\[
\text{Gini}(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
\]</span></p>
<p>容易发现它是一个凸函数。</p>
<p>对于一个集合 D 及其划分 <span
class="math inline">\(C_k\)</span>，类似有</p>
<p><span class="math display">\[
\text{Gini}(D)=\sum_{k=1}^K \dfrac{|C_k|}{|D|}
\left(1-\dfrac{|C_k|}{|D|}\right)=1-\sum_{k=1}^K \left(
\dfrac{|C_k|}{|D|} \right)^2
\]</span></p>
<p>类似条件经验熵函数，也可以定义条件 Gini 系数。</p>
<p><span class="math inline">\(\text{Gini}\)</span>
系数的概念非常类似经验熵函数，同样描述了数据的不确定程度。</p>
<p>分类树则采用 <span class="math inline">\(\text{Gini}\)</span>
系数，对于每一个特征 <span
class="math inline">\(A\)</span>，对其所有可能值 <span
class="math inline">\(a_i\)</span>，进行 y/n 分类，将 <span
class="math inline">\(D\)</span> 划分为 <span
class="math inline">\(D_1\)</span>，<span
class="math inline">\(D_2\)</span> 两部分，计算条件 <span
class="math inline">\(\text{Gini}\)</span>
系数；选择最小的作为分类，递归操作直到满足终止条件。</p>
<h3 id="剪枝">剪枝</h3>
<p>TODO</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
</search>
