<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Computer Networking</title>
    <url>/2026/01/14/Computer-Networking-1-Introduction/Computer-Networking-1-Introduction/</url>
    <content><![CDATA[<h2 id="概览">概览</h2>
<p>我们将网络架构自顶向下地分成 5
层。每一层只需要做好自己的工作就可以了。</p>
<p>由于历史原因，最顶层被叫做 "Layer 7"</p>

<h2 id="layers-of-the-internet">Layers of the Internet</h2>
<h3 id="physical-layer">Physical Layer</h3>
<p>In the Internet, we’re looking for a way to signal bits (1s and 0s)
across space.</p>
<p>The technology could be voltages on an electrical wire, wireless
radio waves, light pulses along optical fiber cables, among others.</p>
<p>Physical Layer 数据的基本单位是<strong>bit</strong>。</p>
<h3 id="link-layer">Link Layer</h3>
<p>Link Layer cares about how to send the data using Physical Layer.</p>
<p>Link Layer
只负责相邻节点之间的通信，其单位是<strong>frame/帧</strong>。</p>
<h3 id="internet-layer">Internet Layer</h3>
<p>Internet Layer 是路由器工作的最高层，负责决定 next
hop，即把数据转发给谁。其基本单位是<strong>packet/包</strong>。</p>
<p>IP 协议就是在这一层工作的。</p>
<h3 id="transport-layer">Transport Layer</h3>
<p>Transport Layer 负责决定数据传输的可靠性，也就是 TCP/UDP
协议，以及“端到端通信”。</p>
<p>也就是说，他通过端口区分应用的数据的来/去。</p>
<p>其基本单位是<strong>段/segment</strong>。</p>
<h3 id="application-layer">Application Layer</h3>
<p>只负责考虑如何使用网络。它的基本数据单位被称作
<strong>message</strong>。</p>
<h2 id="headers">Headers</h2>
<p>每一层都会把从更低层拿来的数据加上对应的 Header。</p>
<p>Header 是只给这一层看的信息（同时，每一层也只能看自己层级的
header），它指明了信息传输的各种附加信息（采取的协议、收件人/发件人等等）。</p>
<p>在传输过程中，Header 不断被 peel off，然后又被加上新的。</p>
<p>由于 Header 的存在，每一层相当于仅仅和自己同层的 peer
进行通信，因此也要求相同层级采用的协议必须相同。</p>



<h2 id="resource-sharing">Resource Sharing</h2>
<p>网络的总容量是有限度的，因此如何分配资源是一个重要的问题。</p>
<p>我们先考虑两个问题：</p>
<ol type="1">
<li><p>要保证网络正常工作，我们至少要多大的容量？</p></li>
<li><p>我们如何分配我们的容量？</p></li>
</ol>
<p>第一个问题的解决方案被称作 statistical
multiplexing，其原理非常直观：</p>


<p>第二个问题有两种解决办法，一种被称作 <strong>best
effort</strong>，通俗而言就是所有人都只管发送信息，并且 "hope for the
best"，其对应的策略被称作 packet switching</p>
<p>另一种被称作
<strong>reservation</strong>，也就是在通信前会先在网络中预留出容量，其对应的策略被称作
Circuit Switching</p>


<h2 id="properties-of-links">Properties of Links</h2>
<p>The <strong>bandwidth</strong> of a link tells us how many bits we
can send on the link per unit time. Intuitively, this is the speed of
the link. If you think of a link as a pipe carrying water, the bandwidth
is the width of the pipe. A wider pipe lets us feed more water into the
pipe per second. We usually measure bandwidth in bits per second (e.g. 5
Gbps = 5 billion bits per second).</p>
<p>The <strong>propagation delay</strong> of a link tells us how long it
takes for a bit to travel along the link. In the pipe analogy, this is
the length of the link. A shorter pipe means that water spends less time
in the pipe before arriving at the other end. Propagation delay is
measured in time (e.g. nanoseconds, milliseconds).</p>
<p>If we multiply the bandwidth and the propagation delay, we get the
<strong>bandwidth-delay product (BDP)</strong>. Intuitively, this is the
<strong>capacity</strong> of the link, or the number of bits that exist
on the link at any given instant. In the pipe analogy, if we fill up the
pipe and freeze time, the capacity of the pipe is how much water is in
the pipe in that instant.</p>
<p>注意，bandwidth/带宽是指“一秒内发射的 bit 数量”，而 bit
必须是一个一个发射出去的。所以一个 bit 的发射用时是 <span
class="math inline">\(\frac{1}{\text{bandwidth}}\)</span>。</p>
<p>注意区分 delay（或者用全称，propagation delay） 和这个发射用时
<strong>（也就是上文的 transmission delay）</strong> 的区别。前者是 bit
在物理链路里面传播(propagation)的时间，后者是终端发送一个 packet 所有
bit 的时间。这从我们下文的例子就可以看出来。</p>

<h3 id="timing-diagram">Timing Diagram</h3>
<p>Suppose we have a link with bandwidth 1 Mbps = 1 million bits per
second, and propagation delay of 1 ms = 0.001 seconds.</p>
<p>We want to send a 100 byte = 800 bit packet along this link. How long
does it take to send this packet, from the time the first bit is sent,
to the time the last bit is received?</p>
<p>To answer this question, we can draw a timing diagram. The left bar
is the sender, and the right bar is the recipient. Time starts at 0 and
increases as we move down the diagram.</p>

<p>Let’s focus on the first bit. We can put 1,000,000 bits on the link
per second (bandwidth), so it takes 1/1,000,000 = 0.000001 seconds to
put a single bit on the link. At time 0.000001 seconds, the link has a
single bit on it, at the sender end.</p>
<p>It then takes 0.001 seconds for this bit to travel across the link
(propagation delay), so at time 0.000001 + 0.001 seconds, the very first
bit arrives at the recipient.</p>

<p>Now let’s think about the last bit. From before, it takes 0.000001 to
put a bit on the link. We have 800 bits to send, so the last bit is
placed on the link at time <span
class="math inline">\(800*0.000001=0.0008\)</span> seconds.</p>
<p>It then takes 0.001 seconds for the last bit to travel across the
link, so at time 0.0008 + 0.001 seconds, the very last bit arrives at
the recipient. This is the time when we can say the packet has arrived
at the recipient.</p>

<h3 id="packet-delay">Packet Delay</h3>
<p>从上文例子可以看出，一个完整的 packet 发送的延迟就是 <span
class="math inline">\(\text{transmission delay + propagation
delay}\)</span></p>
<h3 id="pipe-diagram">Pipe Diagram</h3>
<p>To draw the link, we can imagine the link is a pipe (similar to the
water analogy) and draw the pipe as a rectangle, where the width is the
propagation delay, and the height is the bandwidth. The area of the pipe
is the capacity of the link.</p>




<p>Pipe diagrams can be useful for comparing different links. Let’s look
at the exact same packets traveling through three different links.</p>

<h3 id="overloaded-links">Overloaded Links</h3>
<p>In the long term, we have enough capacity to send all the outgoing
packets, but at this very instant in time, we have two packets arriving
simultaneously, and we can only send out one. This is called transient
overload, and it’s extremely common at switches in the Internet.</p>
<p>To cope with transient overload, the switch maintains a queue of
packets. If two packets arrive simultaneously, the switch queues one of
them and sends out the other one.</p>


<p>At any given time, the switch could choose to send a packet from one
of the incoming links, or send a packet from the queue. This choice is
determined by a packet scheduling algorithm, and there are lots of
different designs that we’ll look at.</p>
<p>Now that we have a notion of queuing, we need to go back and update
our packet delay formula. <strong>Now, packet delay is the sum of
transmission delay, propagation delay, and queuing delay</strong>.</p>
]]></content>
  </entry>
  <entry>
    <title>EM算法</title>
    <url>/2025/12/20/EM%E7%AE%97%E6%B3%95/EM%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h2 id="em-算法的目标">EM 算法的目标</h2>
<p>先考虑著名的三硬币问题： <div class="note primary"><p>假设三枚硬币 A, B, C，其正面朝上的概率分别为 <span
class="math inline">\(\pi,p,q\)</span>。</p>
<p>先抛 A，若正面朝上抛 B，反之则抛 C。</p>
<p>只记录抛 B，C 的结果，得到一个 01 序列。</p>
<p>要求给出此模型的参数 <span
class="math inline">\(\theta=(\pi,p,q)\)</span></p>
</div></p>
<p>抽象出问题实质：</p>
<p>给出观测数据 <span class="math inline">\(Y=[Y_1,\dots,Y_n]^T\)</span>
以及隐变量 <span
class="math inline">\(Z=[Z_1,\dots,Z_n]^T\)</span>，则给出似然估计：
<span class="math display">\[
P(Y|\theta) =\sum_Z P(Y|Z,\theta) P(Z|\theta)
\]</span></p>
<p>此问题中可以化简为：</p>
<p><span class="math display">\[
P(Y|\theta) =\prod_{j=1}^n \left[\pi p^{y_j} (1-p)^{1-y_j} +(1-\pi)
q^{y_j} (1-q)^{1-y_j}  \right]
\]</span> 希望给出</p>
<p><span class="math display">\[
\hat{\theta}=\arg \max_{\theta} \log(P(Y|\theta))
\]</span></p>
<p>下面给出一个用语的约定：</p>
<ul>
<li><p><strong>观测数据</strong>/<strong>不完全数据</strong> <span
class="math inline">\(Y=[Y_1,\dots,Y_n]^T\)</span></p></li>
<li><p>隐变量/<strong>未观测数据</strong>/ <span
class="math inline">\(\text{Latent Variables}\)</span> <span
class="math inline">\(Z=[Z_1,\dots,Z_n]^T\)</span></p></li>
<li><p>完全数据：<span
class="math inline">\(P(Y,Z|\theta)\)</span></p></li>
<li><p>完全数据的对数似然函数：<span class="math inline">\(\log
P(Y,Z|\theta)\)</span></p></li>
</ul>
<h2 id="em-算法的导出">EM 算法的导出</h2>
<p>设 <span class="math display">\[
\begin{align}
L(\theta) &amp;=P(Y|\theta)=\sum_Z P(Y|Z,\theta) P(Z|\theta) \\
&amp;=\log \left( \sum_Z P(Y|Z,\theta) P(Z|\theta) \right)
\end{align}
\]</span></p>
<p>我们逐步迭代去求出最优化的 <span
class="math inline">\(\theta\)</span>，设第 <span
class="math inline">\(i\)</span> 次求出的解是 <span
class="math inline">\(\theta^{(i)}\)</span>，则： <span
class="math display">\[
\begin{align}
L(\theta)-L(\theta^{(i)})&amp;=\log \left( \sum_Z P(Y|Z,\theta)
P(Z|\theta) \right)-\log P(Y|\theta^{(i)})
\\
&amp;= \log \left(\sum_Z P(Z|Y,\theta^{(i)}) \dfrac{  P(Y|Z,\theta)
P(Z|\theta) }
{P(Z|Y,\theta^{(i)})} \right)-\log P(Y|\theta^{(i)})\\
&amp;\geq \sum_Z P(Z|Y,\theta^{(i)})\log \left( \dfrac{  P(Y|Z,\theta)
P(Z|\theta) }
{P(Z|Y,\theta^{(i)})}   \right)-\log P(Y|\theta^{(i)})\\
&amp;= \sum_Z P(Z|Y,\theta^{(i)})\log \left( \dfrac{  P(Y|Z,\theta)
P(Z|\theta) }
{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}   \right)\\
\end{align}
\]</span></p>
<p>不等号是Jensen不等式给出的</p>
<p>设 <span
class="math inline">\(B(\theta,\theta^{(i)})\hat{=}L(\theta^{(i)})+\sum_Z
P(Z|Y,\theta^{(i)})\log \left( \dfrac{ P(Y|Z,\theta) P(Z|\theta)
}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})} \right)\)</span></p>
<p>显然有 <span class="math inline">\(L(\theta)\geq
B(\theta,\theta^{(i)})\)</span>，即 <span
class="math inline">\(B\)</span> 为原本函数的一个下界，且 <span
class="math inline">\(\theta=\theta^{(i)}\)</span> 时取等。
故可以考虑最大化 <span class="math inline">\(B\)</span>，从而使得 <span
class="math inline">\(L\)</span> 最大。</p>
<p>即： <span class="math display">\[
\begin{align}
\theta^{(i+1)} &amp;= \arg \max _{\theta} B(\theta,\theta^{(i)})  \\
&amp;=\arg \max_{\theta} \left( L(\theta^{(i)})+\sum_Z
P(Z|Y,\theta^{(i)})\log \left( \dfrac{  P(Y|Z,\theta) P(Z|\theta)
}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}   \right) \right)
\\
&amp;=\arg \max_{\theta} \left( \sum_Z P(Z|Y,\theta^{(i)})\log
\left(   P(Y|Z,\theta) P(Z|\theta) \right) \right)\\
&amp;=\arg \max_{\theta} \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)
\end{align}
\]</span> 上面的等号都是去掉了常数项（上面的变量只有 <span
class="math inline">\(\theta\)</span>，<span
class="math inline">\(\theta^{(i)}\)</span>固定） 令 <span
class="math inline">\(Q(\theta,\theta^{(i)})=\sum_Z
P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)\)</span>，则 Q 函数即为：</p>
<p>完全似然函数 <span class="math inline">\(\log P(Y,Z|\theta)\)</span>
关于在给定观测数据 <span class="math inline">\(Y\)</span>
和当前参数估计<span class="math inline">\(\theta^{(i)}\)</span>
下，对于未观测数据 <span class="math inline">\(Z\)</span> 的条件概率分布
<span class="math inline">\(P(Z|Y,\theta^{(i)})\)</span> 的期望。</p>
<p>EM 算法实质上就是找一个凸函数 <span
class="math inline">\(B/Q\)</span>，让它在 <span
class="math inline">\(\theta^{(i)}\)</span> 处严格等于对数似然函数 <span
class="math inline">\(\log
P(Y,Z|\theta)\)</span>，但始终在似然函数下方。我们通过不断增大 B
函数，迫使似然函数上升。</p>
<div class="note info"><p>关于 Q 函数更直观的认知，见 <a
href="https://chat.qwen.ai/c/41591c41-69ab-4005-8cbd-a9db5412b27b">Qwen</a></p>
</div>
<h2 id="em-算法的流程">EM 算法的流程</h2>
<ol type="1">
<li>初始化 <span class="math inline">\(\theta^{(0)}\)</span></li>
<li>E(Expectation): 求出 Q 函数</li>
<li>M(Maximization): 最大化 Q 函数，求出下一轮的 <span
class="math inline">\(\theta^{(i+1)}\)</span></li>
<li>重复 E-M 步，直到收敛为止</li>
</ol>
<h2 id="gmm">GMM</h2>
<p>GMM，即 Gaussian Mixture Model，刻画了如下的问题： <div class="note "><p>有 K 个 Gaussian，分别服从 <span
class="math inline">\(\theta_k=(\mu_k,\sigma_k)\)</span>；第 <span
class="math inline">\(k\)</span> 个模型被选择的概率是 <span
class="math inline">\(\alpha_k\)</span></p>
<p>即 <span class="math inline">\(P(y)=\sum_{k=1}^K \alpha_k
\phi(\mu_k,\sigma_k)\)</span></p>
</div></p>
<p>现在你既不知道各个Gaussian的具体参数，也不知道选择的概率 <span
class="math inline">\(\alpha_k\)</span>，任务就是根据观测的序列 <span
class="math inline">\(y=(y_1,\dots,y_N)\)</span> 求出 <span
class="math inline">\(\theta=(\alpha_1,\dots,\alpha_K;\theta_1,\dots,\theta_K)\)</span></p>
<p>使用 EM 算法完成这个任务，逐步分析：</p>
<h3 id="明确隐变量">明确隐变量</h3>
<p>设 <span class="math display">\[
\gamma_{jk}=
\begin{cases}
1 &amp; 第 j 个观测来自第 k 个模型\\
0 &amp; 反之
\end{cases}
\]</span></p>
<p>有了观测数据 <span class="math inline">\(y_j\)</span>
之后，那么完全数据就是</p>
<p><span class="math display">\[
(y_j,\gamma_{j1},\dots,\gamma_{jk})
\]</span></p>
<p>后面整体算作上文的 <span class="math inline">\(Z\)</span></p>
<p>写出似然函数：</p>
<p>$$ <span class="math display">\[\begin{align}
P(y,\gamma|\theta) &amp;= \prod_{j=1}^N
P(y_j,\gamma_{j1}\dots,\gamma_{jk})\\

&amp;= \prod_{j=1}^N \prod_{k=1}^K [\alpha_k
\phi(y_j|\theta_k)]^{\gamma_{jk}}\\

\end{align}\]</span> $$</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>EM 算法</tag>
      </tags>
  </entry>
  <entry>
    <title>2026新年展望</title>
    <url>/2026/01/01/2026%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/2026%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/</url>
    <content><![CDATA[<p>2026 就这么突然来了。</p>
<p>我已经很久没有写什么非技术性的东西了，只能感叹这一年过的比我体感要快得多。</p>
<p>2025 的一月仿佛还历历在目。在鼓楼图书馆里看 FSF，百无聊赖地复习着 CPL
和微积分；南京清早熙熙攘攘的人声，冬日清朗的空气，教超的蛋糕和布丁仿佛还是几天前的事情。</p>
<p>是因为长大一点了吗，对于过去的事情记得更加清楚，但时间也感觉流动的更快了。</p>
<p>去年寒假看了一半的
CSAPP，到现在还没看完。学了一点皮毛的数学分析，只有在看花书的时候派上了点用场。CS61A
倒是学完了，但是感觉这门课属于会的人不需要听，不会的人一开始也难解其妙。</p>
<p>先争取活过这个无比劳碌的期末周吧。</p>
<p>寒假里最重要的还是在学校里老老实实搬砖，争取今年能把第一篇文章投中（能中
TPAMI 什么的就最好了）。</p>
<p>然后完成一些一直想学，但是也没时间学的东西吧：</p>
<ol type="1">
<li><p>UCB CS186 Database System：正好学学 Java，万一将来不去做 AI
去做互联网码农也能用得上。
其实我一直对软工系的东西很感兴趣，但苦于课程安排一直没什么机会。</p></li>
<li><p>UCB CS168/Stanford CS144 Computer
Networking：其实早在一年前就听完了 USTC
的计网，但是当时也就听个响，现在脑海里也就一点粗略的印象。
看网上的建议，听 UCB 的课，然后把两个的实验都做了。
还是得动手写点代码：（</p></li>
<li><p>把拖了一年的 CSAPP 听完，然后把实验做了。其实我很想做计科的
ICSPA，但是不知道有没有时间，估计要做的话只有过年的时间能砸进去了</p></li>
<li><p>操作系统。这个还没决定是 MIT 的还是 UCB
的，先把上面的七七八八做完再考虑这个吧。</p></li>
<li><p>学学 JavaScript</p></li>
<li><p>把《统计学习方法》之前跳过的部分读完。这学期学了凸优化，应该算是补上了
AI 需要的数学中的最后一块了。</p></li>
</ol>
<p>这个学期都没写什么代码。事实上，整个一年都没写什么代码，但是理论貌似学的也不怎么样。</p>
<p>还是希望能提升一下自己的动手能力，亲自做一个大一点的工程吧。</p>
<p>再见，2025；干杯，2026！</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title>Hexo Blog 的一些技术细节</title>
    <url>/2025/12/20/Hexo%20Blog%20%E7%9A%84%E4%B8%80%E4%BA%9B%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82/index/</url>
    <content><![CDATA[<p>花了一些时间总算搞好了，第一篇文章就简要记录一下踩的坑吧。</p>
<h2 id="数学公式渲染">数学公式渲染</h2>
<p>处于兼容性考虑，我采用了 <code>Mathjax</code> 作为 math
engine，同时将默认 <code>Markdown</code> 渲染器换成了
<code>pandoc</code></p>
<p>一定要把之前的渲染引擎删除干净，否则会有一些很诡异的错误</p>
<h2 id="英文引号变成中文">英文引号变成中文</h2>
<p>我想写这篇文章的罪魁祸首。</p>
<p>一开始，所有的英文引号 <code>'/"</code> 都会被渲染成
<code>‘/“</code>，显示效果奇差无比</p>
<p>查询之后发现这也是渲染引擎的问题，它会自动开启一个类似于 formatting
之类的操作</p>
<p>Github 上比较常见的几个 <a
href="https://github.com/hexojs/hexo/issues/1981">issue</a> 都是用的
<code>hexo-renderer-marked</code></p>
<p>但是如上文所说，我换成了 <code>pandoc</code>
之后依然有这个问题（有的文章说换 <code>pandoc</code>
之后就好了），猜测也有一个类似的开关</p>
<p>随问 LLM，发现要传入一些参数： <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">pandoc:</span></span><br><span class="line">  <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;--from&#x27;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;markdown-smart&#x27;</span> <span class="comment"># 这里的 -smart 表示在 markdown 解析中禁用 smart 扩展</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&#x27;--mathjax&#x27;</span></span><br></pre></td></tr></table></figure></p>
<p>我估计可能是因为 pandoc 版本的不同，旧版本的默认关闭/没有 smart
扩展</p>
]]></content>
      <categories>
        <category>Others</category>
      </categories>
  </entry>
  <entry>
    <title>First Blog</title>
    <url>/2025/12/19/First-Blog/First-Blog/</url>
    <content><![CDATA[<p>Hello! This is my first blog.</p>
<p>A math formula: <span class="math display">\[
\frac{\partial p_t(\mathbf{x})}{\partial t} = -\sum_{i=1}^{D}
\frac{\partial}{\partial x_i} \left[ f_i(\mathbf{x}, t) p_t(\mathbf{x})
\right] + \frac{1}{2} \sum_{i=1}^{D} \sum_{j=1}^{D}
\frac{\partial^2}{\partial x_i \partial x_j} \left[ (g^2(t)
\mathbf{I}_D)_{ij} p_t(\mathbf{x}) \right]
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;This is a piece of code!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>尝试一下中文输入 This's my blog!</p>
<ul>
<li>todo 1</li>
<li>todo 2</li>
</ul>
<h1 id="hello">Hello</h1>
<h2 id="你好">你好</h2>
<h3 id="foo"><code>foo</code></h3>
<h4 id="geq-2"><span class="math inline">\(1+2\geq 2\)</span></h4>
<p>有<span class="math inline">\(a+b\geq 3\)</span></p>
<p><span class="math display">\[a+b\geq 3\]</span></p>
]]></content>
  </entry>
  <entry>
    <title>Mamba Architecture</title>
    <url>/2025/12/24/Mamba-Architecture/Mamba-Architecture/</url>
    <content><![CDATA[<p>Mamba 是一种 State Space
Model/SSM，即状态空间模型。这是用以处理序列问题的一种非常优秀的模型。</p>
<p>其本质可以理解为“隐空间”/“状态空间”中的递推。</p>
<h2 id="问题">问题</h2>
<p>给定时间信号输入序列 <span
class="math inline">\(x(t)\)</span>，以及输出序列 <span
class="math inline">\(y(t)\)</span>，尝试给出预测模型。
我们考虑一个隐状态空间 <span class="math inline">\(h(t)\in
\mathbb{R}^N\)</span>，它由 <span class="math inline">\(x(t)\)</span>
而来，控制了 <span class="math inline">\(y(t)\)</span> 的输出。
从简化的角度考虑，给出线性模型：</p>
<p><span class="math display">\[\begin{align}
h&#39;(t) &amp;= \mathbf{A}h(t)+\mathbf{B}x(t) \\
y(t)  &amp;= \mathbf{C}h&#39;(t)
\end{align}
\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{A}\)</span>，<span
class="math inline">\(\mathbf{B}\)</span>，<span
class="math inline">\(\mathbf{C}\)</span> 三个矩阵分别代表 evolution
parameter 和 projection
parameter。第一个的作用是将隐空间中的状态逐步演化的最终态，后二者的作用是进行隐空间和实际输入/输出空间的映射。</p>
<h2 id="从连续时间到离散时间">从连续时间到离散时间</h2>
<p>上面的模型显然是一个连续时间模型，也就意味着是一个微分方程。</p>
<p>但是在实际工程和应用中，我们不可能去解一个微分方程，因此我们考虑去将他离散化。其中，最重要的一种离散化方式被称作
"Zero-Order Hold/ZOH" 规则。</p>
<h3 id="zoh">ZOH</h3>
<p>ZOH 的核心假设非常易于理解：</p>
<div class="note "><h4 id="zero-order-hold">Zero-Order Hold</h4>
<p>在每个采样区间中，输入保持常数：</p>
<p><span class="math display">\[
u(t)=u_k\quad t\in [t_k,t_{k+1})
\]</span></p>
<p>其中，采样步长 <span
class="math inline">\(\Delta&gt;0\)</span>，<span
class="math inline">\(t_k=k\Delta\)</span></p>
</div>
<p>我们希望得到离散递推公式： <span class="math display">\[
x_{k+1}=\bar{A}x_k+\bar{B}u_k
\]</span></p>
<p>所以，也就是去解这个微分方程（和普通的微分方程一样，先不考虑是矩阵的情况）：
<span class="math display">\[
\frac{\mathrm{d}x}{\mathrm{d}t}=Ax+Bu
\]</span></p>
<p>注意，这里 <span class="math inline">\(x\)</span> 是状态，<span
class="math inline">\(u\)</span> 是输入。</p>
<p>将系统重写为 <span class="math display">\[
\dot{x}(t)-Ax(t)=Bu(t)
\]</span></p>
<p>解之即有： <span class="math display">\[
\bar{A}=e^{\Delta
A},\bar{B}=\int_{t_k}^{t_{k+1}}e^{A(t_{k+1}-\tau)}B\mathrm{d}\tau
\]</span></p>
<p>若 <span class="math inline">\(A\)</span> 可逆，则 <span
class="math inline">\(B\)</span> 有闭式解 <span
class="math inline">\(A^{-1}(e^{A\Delta}-I)B\)</span></p>
<div class="note info"><h4 id="矩阵指数">矩阵指数</h4>
<p><span class="math inline">\(B=e^{A}\)</span> 并不是 <span
class="math inline">\(B_{ij}=\exp\{a_{ij}\}\)</span>，而是采用泰勒展开：
<span class="math display">\[
e^A=\sum_{i=0}^{\infin} \frac{A^k}{k!}
\]</span></p>
<p>对于对角矩阵，退化为上面的直觉的形式： <span class="math display">\[
A=\text{diag}(a_1,\dots,a_n) \Rightarrow
e^A=\text{diag}(e^{a_1},\dots,e^{a_n})
\]</span></p>
<p>类似，对于可对角化矩阵，也有： <span class="math display">\[
A=P \text{diag}(a_1,\dots,a_n) P^{-1} \Rightarrow
e^A=P\text{diag}(e^{a_1},\dots,e^{a_n})P^{-1}
\]</span></p>
</div>
<h3 id="递推">递推</h3>
<p>基于 ZOH 假设，我们可以快速写出以下式子（<span
class="math inline">\(M\)</span> 是序列长度）： <span
class="math display">\[
\begin{align}
\bar{K} &amp;= (C\bar{B}, C\bar{A}\bar{B}, C\bar{A}^2\bar{B},\dots,
C\bar{A}^{M-1}\bar{B})\\
y &amp;= x * \bar{K}
\end{align}
\]</span></p>
<h2 id="一个-mamba-block">一个 Mamba Block</h2>
<p>一个 Mamba Block
做一次完整的我们上述的递推操作，也就是对于每个位置提取一次上下文理解。</p>
<p>和 Transformer 类似，我们也可以把多个 Mamba Block
叠起来，进行更深度的信息提取，也就是<strong>逐层抽象</strong>，越靠后的
Mamba Block 蕴含了越丰富的、越抽象的信息。</p>
<p><span class="math display">\[
x^{(0)}\xrightarrow{\text{Mamba Block 1}}
x^{(1)}\xrightarrow{\text{Mamba Block 2}}\dots \xrightarrow{\text{Mamba
Block M}} x^{(M)}
\]</span></p>
<h2 id="具体实现">具体实现</h2>
<h3 id="algorithm-1parameters-function">Algorithm 1：Parameters
Function</h3>
<p>根据我们上文的描述，从给定的序列 <span
class="math inline">\(\{x\}\)</span> 中生成 <span
class="math inline">\(\bar{A}, \bar{B}, C\)</span>。</p>
<p><span class="math display">\[
\begin{aligned}
\textbf{Algorithm 1: Parameters Function}\\
\textbf{Require: } x&#39; \in \mathbb{R}^{(B,N,P)}\\
\textbf{Ensure: } \bar A \in \mathbb{R}^{(B,N,P,K)},\ \bar B \in
\mathbb{R}^{(B,N,P,K)},\ C \in \mathbb{R}^{(B,N,K)}\\[4pt]
\begin{array}{ll}
1: &amp; B \in \mathbb{R}^{(B,N,K)} \leftarrow
\mathrm{Linear}^{B}(x&#39;)\\
2: &amp; C \in \mathbb{R}^{(B,N,K)} \leftarrow
\mathrm{Linear}^{C}(x&#39;)\\
3: &amp; \Delta \in \mathbb{R}^{(B,N,P)} \leftarrow
\log\!\Big(1+\exp(\mathrm{Linear}^{\Delta}(x&#39;)+\mathrm{Parameter}^{\Delta})\Big)\\
4: &amp; \text{// } \mathrm{Parameter}^{A} \in \mathbb{R}^{(P,K)}\\
5: &amp; \bar A \in \mathbb{R}^{(B,N,P,K)} \leftarrow \Delta \otimes
\mathrm{Parameter}^{A}\\
6: &amp; \bar B \in \mathbb{R}^{(B,N,P,K)} \leftarrow \Delta \otimes B\\
\textbf{Return:} &amp; \bar A,\ \bar B,\ C
\end{array}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\textbf{Algorithm 2: Mamba Block}\\
\textbf{Require: } T_{l-1} \in \mathbb{R}^{(B,N,C)}\\
\textbf{Ensure: } T_{l} \in \mathbb{R}^{(B,N,C)}\\[4pt]
\begin{array}{ll}
1: &amp; \text{// Apply layer normalization to } T_{l-1}\\
2: &amp; T&#39;_{l-1} \in \mathbb{R}^{(B,N,C)} \leftarrow
\mathrm{Norm}(T_{l-1})\\
3: &amp; x \in \mathbb{R}^{(B,N,P)} \leftarrow
\mathrm{Linear}^{x}(T&#39;_{l-1})\\
4: &amp; z \in \mathbb{R}^{(B,N,P)} \leftarrow
\mathrm{Linear}^{z}(T&#39;_{l-1})\\
5: &amp; \text{// Process the input sequence}\\
6: &amp; x&#39; \in \mathbb{R}^{(B,N,P)} \leftarrow
\mathrm{SiLU}(\mathrm{Conv1d}(x))\\
7: &amp; \bar A,\ \bar B,\ C \leftarrow
\mathrm{ParametersFunction}(x&#39;)\\
8: &amp; y \in \mathbb{R}^{(B,N,P)} \leftarrow \mathrm{SSM}(\bar A,\bar
B,C)(x&#39;)\\
9: &amp; \text{// Obtain gated } y\\
10: &amp; y&#39; \in \mathbb{R}^{(B,N,P)} \leftarrow y \odot
\mathrm{SiLU}(z)\\
11: &amp; \text{// Residual connection}\\
12: &amp; T_{l} \in \mathbb{R}^{(B,N,C)} \leftarrow
\mathrm{Linear}^{T}(y&#39;) + T_{l-1}\\
\textbf{Return:} &amp; T_{l}
\end{array}
\end{aligned}
\]</span></p>
]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Mamba</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2025/12/20/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h2 id="决策树学习算法">决策树学习算法</h2>
<p>决策树的学习算法一共有三点： <div class="note primary"><p>特征选择，决策树生成，决策树剪枝</p>
</div></p>
<p>决策树的非叶子节点代表选择的一个划分标准（如，第 <span
class="math inline">\(i\)</span> 个分量 <span
class="math inline">\(x^{(i)}\)</span> 是否大于等于 <span
class="math inline">\(0\)</span>），每一个叶子节点代表一个类（样本属于这些类）</p>
<h2 id="特征选择">特征选择</h2>
<p>希望每一步选择的特征都是良好的，引入一个指标来计算：信息增益。</p>
<p>信息增益：得知 特征X 的信息而使得类 Y
的信息的不确定性减少的程度，即经验熵的改变量。 定义为： <span
class="math display">\[
g(D, A)=H(D)-H(D|A)
\]</span> 其中 <span class="math inline">\(D\)</span> 为数据集，<span
class="math inline">\(A\)</span> 为我们选择的特征，<span
class="math inline">\(H\)</span> 为经验熵函数。</p>
<p>我们希望 <span class="math inline">\(H\)</span> 越小越好，即每一步的
<span class="math inline">\(g(D,A)\)</span> 要大。</p>
<p>容易发现，当 <span class="math inline">\(A\)</span>
表征的特征有更多选项时更容易被选中，所以引入信息增益比： <span
class="math display">\[
g_R(D, A)=\dfrac{g(D,A)}{H_A(D)}
\]</span> 其中 <span class="math inline">\(H_A(D)\)</span> 为数据集
<span class="math inline">\(D\)</span> 关于特征 <span
class="math inline">\(A\)</span> 的熵 <span class="math display">\[
H_A(D)=-\sum_{i=1}^n \dfrac{|D_i|}{|D|}\log_2{\dfrac{|D_i|}{|D|}}
\]</span></p>
<h2 id="决策树生成">决策树生成</h2>
<p>一共两种算法，ID3 和 C4.5</p>
<p>ID3 算法依赖于信息增益，而 C4.5 则是信息增益比作为评价标准</p>
<p>流程：</p>
<ol type="1">
<li><p>给定数据集 <span class="math inline">\(D\)</span> 以及特征集
<span class="math inline">\(A\)</span> 阈值 <span
class="math inline">\(\epsilon\)</span></p></li>
<li><p>按照每一个特征，计算信息增益（比）；选取信息增益（比）最大的特征
<span class="math inline">\(A\)</span>，如果节点全部都属于同一类 <span
class="math inline">\(C_k\)</span>，则直接标记为<span
class="math inline">\(C_k\)</span> 类并返回</p></li>
<li><p>否则开始划分，按照 <span class="math inline">\(A\)</span>
的每一个可能的值 <span class="math inline">\(a_i\)</span> 将划分为 <span
class="math inline">\(D_i\)</span>，递归此过程，直到子节点全部属于一类或者信息增益（比）小于阈值
<span class="math inline">\(\epsilon\)</span> 或者特征集 <span
class="math inline">\(A\)</span> 为空。</p></li>
</ol>
<p>这两个都容易产生过拟合的问题，故考虑剪枝</p>
<h2 id="决策树剪枝">决策树剪枝</h2>
<p>实质上就是一种正则化手段，利用经验熵作为损失函数，叶子节点数作为正则化项。</p>
<p>定义决策树学习的损失函数为：</p>
<p><span class="math display">\[
C_\alpha(T)=\sum_{t=1}^{|T|} N_tH_t(T)+\alpha|T|
\]</span></p>
<p>其中经验熵函数为</p>
<p><span class="math display">\[
H_t(T)=-\sum_k \dfrac{N_{tk}}{N_t}\log \dfrac{N_{tk}}{N_t}
\]</span></p>
<p>其中，<span class="math inline">\(|T|\)</span>
为决策树的节点数（正则化项），<span class="math inline">\(t\)</span>
是子节点编号，<span class="math inline">\(N_t\)</span>
是每个叶节点的样本数，其中第 <span class="math inline">\(k\)</span>
类的有 <span class="math inline">\(N_{tk}\)</span> 个。</p>
<p>当 <span class="math inline">\(\alpha\)</span>
很小的时候，说明选择损失函数最小的模型，调大 <span
class="math inline">\(\alpha\)</span> 相当于限制（减小）模型复杂度。</p>
<p>剪枝算法： 1. 递归，计算每一个叶节点的经验熵</p>
<ol start="2" type="1">
<li><p>尝试将一个叶子节点缩到父节点，比较前后的经验熵，如果经验熵减小，说明可以直接剪枝，将父节点变为一个新的叶节点。</p></li>
<li><p>回到2，继续操作</p></li>
</ol>
<h2 id="cart-决策树">CART 决策树</h2>
<h3 id="概述">概述</h3>
<p>CART 是一种二叉决策树，每次提问都是 y/n；</p>
<p>对于回归问题，使用平方误差作为损失函数；对于分类问题，使用 <span
class="math inline">\(\text{Gini}\)</span> 系数；</p>
<h3 id="模型">模型</h3>
<h4 id="回归树">回归树</h4>
<p><span class="math display">\[
f(x)=\sum_{m=1}^M c_m \mathbb{I}(x\in R_m)
\]</span></p>
<p>其中，<span class="math inline">\(R_m\)</span>
为提问所划分出的区域，<span class="math inline">\(c_m\)</span>
为其上的代表值（固定的输出值）；</p>
<p>通过平方误差计算时，容易知道 <span class="math inline">\(c_m\)</span>
应该取 <span class="math inline">\(R_m\)</span> 的标签的平均值</p>
<p>现在问题在于如何划分。</p>
<p>采用启发式的方法，每次取一个方向 <span
class="math inline">\(x^{(j)}\)</span>，考虑将 D
按照这个方向上的一个数值 <span class="math inline">\(s\)</span>
来划分</p>
<p><span class="math display">\[
R_1(j,s)=\{x|x^{(j)}\leq s\},R_2(j,s)=\{x|x^{(j)}&gt; s\}
\]</span></p>
<p>现在来求解</p>
<p><span class="math display">\[
\min_{j,s} \left\{ \min_{c_1} \sum_{x_i\in R_1(j,s)} (y_i-c)^2 +
\sum_{x_i\in R_2(j,s)} (y_i-c)^2 \right\}
\]</span> 固定 <span class="math inline">\(j\)</span> 有 <span
class="math inline">\(\hat{c}_1=\text{ave}(y_i|x_i\in
R_1(j,s)),\hat{c}_2=\text{ave}(y_i|x_i\in R_2(j,s))\)</span></p>
<p>重复这个过程，直到达到终止条件。</p>
<h4 id="分类树">分类树</h4>
<p>定义 <span class="math inline">\(\text{Gini}\)</span>
系数，对于一个概率分布 <span class="math inline">\(p\)</span> 有</p>
<p><span class="math display">\[
\text{Gini}(p)=\sum_{k=1}^K p_k(1-p_k)=1-\sum_{k=1}^K p_k^2
\]</span></p>
<p>容易发现它是一个凸函数。</p>
<p>对于一个集合 D 及其划分 <span
class="math inline">\(C_k\)</span>，类似有</p>
<p><span class="math display">\[
\text{Gini}(D)=\sum_{k=1}^K \dfrac{|C_k|}{|D|}
\left(1-\dfrac{|C_k|}{|D|}\right)=1-\sum_{k=1}^K \left(
\dfrac{|C_k|}{|D|} \right)^2
\]</span></p>
<p>类似条件经验熵函数，也可以定义条件 Gini 系数。</p>
<p><span class="math inline">\(\text{Gini}\)</span>
系数的概念非常类似经验熵函数，同样描述了数据的不确定程度。</p>
<p>分类树则采用 <span class="math inline">\(\text{Gini}\)</span>
系数，对于每一个特征 <span
class="math inline">\(A\)</span>，对其所有可能值 <span
class="math inline">\(a_i\)</span>，进行 y/n 分类，将 <span
class="math inline">\(D\)</span> 划分为 <span
class="math inline">\(D_1\)</span>，<span
class="math inline">\(D_2\)</span> 两部分，计算条件 <span
class="math inline">\(\text{Gini}\)</span>
系数；选择最小的作为分类，递归操作直到满足终止条件。</p>
<h3 id="剪枝">剪枝</h3>
<p>TODO</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>决策树</tag>
      </tags>
  </entry>
</search>
